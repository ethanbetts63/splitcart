# Decoupling Plan: Semantic Analysis

## 1. Objective

This plan outlines the process for decoupling the computationally intensive semantic analysis tasks (which depend on `torch` and `sentence-transformers`) from the main web application. The goal is to move this processing to a local machine, reducing the server's resource requirements and simplifying its dependencies.

## 2. Current Implementation

- **Category Linking:** The `update --category-links` management command uses `ExactCategoryMatcher` to find semantically similar categories across different stores.
- **Product Substitutions:** The `generate_subs --lvl3` and `generate_subs --lvl4` management commands use `Lvl3SubstitutionGenerator` and `Lvl4SubstitutionGenerator` to find semantically similar products.

These commands are executed on the server and require `torch` and `sentence-transformers` to be installed.

## 3. Proposed Decoupled Architecture

### 3.1. Local Processing Workflow

A new management command, `generate_semantic_data`, will be created for local execution. This command will not be run on the production server.

1.  **Fetch Data:** The script will make API calls to the production server to download the latest product and category data.
2.  **Perform Analysis:** It will execute the existing semantic analysis logic from the current `ExactCategoryMatcher`, `Lvl3SubstitutionGenerator`, and `Lvl4SubstitutionGenerator` classes.
3.  **Generate Output Files:** Instead of writing directly to a database, the script will save the results into two JSON files:
    - `category_links.json`: Contains the pairs of category IDs that should be linked.
    - `substitutions.json`: Contains the pairs of product IDs that should be created as substitutions, along with the level and score.

### 3.2. Server-Side (Production) Workflow

The production server will be modified to provide data to and ingest results from the local process. `torch` and `sentence-transformers` will be removed from the production `requirements.txt`.

1.  **Data Export Endpoints:** New API endpoints will be created to provide the necessary data to the local script:
    - `api/export/products/`: Returns a JSON list of all products.
    - `api/export/categories/`: Returns a JSON list of all categories.
2.  **Data Import Endpoint:** A new API endpoint will be created to receive the generated data:
    - `api/import/semantic_data/`: This endpoint will accept the `category_links.json` and `substitutions.json` files, validate the data, and then create the `CategoryLink` and `Substitution` records in the database.
3.  **Command Refactoring:** The existing `update` and `generate_subs` commands will be modified to remove the calls to the `torch`-dependent classes.

## 4. Relevant Files

- `data_management/management/commands/update.py`
- `data_management/management/commands/generate_subs.py`
- `data_management/database_updating_classes/exact_category_matcher.py`
- `data_management/utils/substitution_utils/lvl3_substitution_generator.py`
- `data_management/utils/substitution_utils/lvl4_substitution_generator.py`
- `requirements.txt`
