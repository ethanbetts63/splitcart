This has been a long and highly productive session focused on performance optimization, bug fixing, and implementing a new data retention strategy.

### 1. Barcode Scraper Enhancements

We began by investigating the barcode scraping process. This led to two key improvements:

*   **`--dev` Flag Implementation:** The `scrape_barcodes` management command was missing a `--dev` flag to target the local API server. I implemented this by adding the argument to the command, passing it through the `ColesBarcodeScraper` class, and updating the `prefill_barcodes_from_api` function to use the local server URL when the flag is present.
*   **Progress File Logic:** You observed that products with prefilled barcodes were not being logged to the `.progress` file, causing them to be re-processed unnecessarily on script restarts. I fixed this by modifying the `get_work_items` method in `barcode_scraper_coles.py` to immediately write a record to the `.progress` file for any product that is completed during the prefill stage.

### 2. Substitution Generator Performance Overhaul

We then systematically optimized all the substitution generators, which were identified as being very slow.

*   **Lvl3 & Lvl4 Generators:** These generators were slow because they loaded a large `SentenceTransformer` model on every run and encoded product names inefficiently within a loop. I refactored both `Lvl3SubGenerator` and `Lvl4SubGenerator` to load the model only once upon initialization and to perform a single, highly-optimized batch encoding of all product names. I also added `x/y` progress indicators to their main processing loops.
*   **Lvl2 Generator:** You confirmed this generator was also slow. I analyzed it and found its bottleneck was a CPU-intensive string comparison algorithm. I optimized it by implementing a "blocking" strategy that dramatically reduces the number of comparisons by first grouping products by the first word of their name. I also added progress reporting to this generator.
*   **Progress Bar Simplification:** You found the default `sentence-transformers` progress bar to be "overkill". I removed the `show_progress_bar=True` argument from the `.encode()` calls in the Level 3 and 4 generators for a cleaner console output.

### 3. Debugging the `source` Field Conflict

After optimizing the generators, we encountered a series of errors during the `update --subs` process.

1.  The first error was `unexpected keyword arguments: 'source'`, because the newly optimized generators were adding a `source` key that the `ProductSubstitution` database model did not have.
2.  You decided against adding the field to the database. I therefore removed the `source` key from the output of all four substitution generators (`Lvl1`, `Lvl2`, `Lvl3`, `Lvl4`).
3.  This led to the opposite error: `Missing key 'source'`. I diagnosed that the `SubstitutionUpdateOrchestrator` was hardcoded to expect the `source` key.
4.  I resolved this by modifying the orchestrator, removing all logic that attempted to read, compare, or save the `source` field, bringing the data consumer in sync with the data generator.

### 4. Debugging the `bargains_generator`

The `generate --bargains` command was also suffering from several issues.

*   **Progress Output:** I updated the paginated fetcher to use a single, updating line for progress instead of printing a new line for every page.
*   **Infinite Loop:** You reported that the process was fetching millions of prices in an infinite loop. I diagnosed this as a critical bug in the `ExportPricesView` API. A default ordering on the `Price` model (`ordering = ['product__name']`) was conflicting with the `CursorPagination` logic, causing the paginator to get stuck in a loop.
*   **The Fix:** I fixed this by adding `.order_by('id')` to the queryset in `ExportPricesView`, which overrides the problematic default and gives the paginator the stable, unique ordering it requires to function correctly.

### 5. New Data Retention Strategy: Price Deletion

After using a temporary `count_prices` command (which I created and then enhanced with per-store counts at your request) to confirm the database contained over 3.7 million price records, you proposed a new strategy to delete redundant prices.

*   **The Plan:** We planned to delete prices for non-anchor stores at two key moments: after a successful merge and after a successful health check.
*   **Implementation:**
    1.  I modified `intergroup_comparer.py` to delete all prices for all stores from a smaller group after it has been successfully merged into a larger one.
    2.  I modified `internal_group_health_checker.py` to delete all prices for a member store after a health check confirms its prices match its anchor.
*   **Documentation:** Finally, I updated the `grouping_strategy.txt` file to document this new, aggressive data retention policy.