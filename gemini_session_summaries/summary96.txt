This has been a long and highly productive session focused on performance optimization, bug fixing, and implementing a new data retention strategy.

### 1. Barcode Scraper Enhancements

We began by investigating the barcode scraping process. This led to two key improvements:

*   **`--dev` Flag Implementation:** The `scrape_barcodes` management command was missing a `--dev` flag to target the local API server. I implemented this by adding the argument to the command, passing it through the `ColesBarcodeScraper` class, and updating the `prefill_barcodes_from_api` function to use the local server URL when the flag is present.
*   **Progress File Logic:** You observed that products with prefilled barcodes were not being logged to the `.progress` file, causing them to be re-processed unnecessarily on script restarts. I fixed this by modifying the `get_work_items` method in `barcode_scraper_coles.py` to immediately write a record to the `.progress` file for any product that is completed during the prefill stage.

### 2. Substitution Generator Performance Overhaul

We then systematically optimized all the substitution generators, which were identified as being very slow.

*   **Lvl3 & Lvl4 Generators:** These generators were slow because they loaded a large `SentenceTransformer` model on every run and encoded product names inefficiently within a loop. I refactored both `Lvl3SubGenerator` and `Lvl4SubGenerator` to load the model only once upon initialization and to perform a single, highly-optimized batch encoding of all product names. I also added `x/y` progress indicators to their main processing loops.
*   **Lvl2 Generator:** You confirmed this generator was also slow. I analyzed it and found its bottleneck was a CPU-intensive string comparison algorithm. I optimized it by implementing a "blocking" strategy that dramatically reduces the number of comparisons by first grouping products by the first word of their name. I also added progress reporting to this generator.
*   **Progress Bar Simplification:** You found the default `sentence-transformers` progress bar to be "overkill". I removed the `show_progress_bar=True` argument from the `.encode()` calls in the Level 3 and 4 generators for a cleaner console output.

### 3. Debugging the `source` Field Conflict

After optimizing the generators, we encountered a series of errors during the `update --subs` process.

1.  The first error was `unexpected keyword arguments: 'source'`, because the newly optimized generators were adding a `source` key that the `ProductSubstitution` database model did not have.
2.  You decided against adding the field to the database. I therefore removed the `source` key from the output of all four substitution generators (`Lvl1`, `Lvl2`, `Lvl3`, `Lvl4`).
3.  This led to the opposite error: `Missing key 'source'`. I diagnosed that the `SubstitutionUpdateOrchestrator` was hardcoded to expect the `source` key.
4.  I resolved this by modifying the orchestrator, removing all logic that attempted to read, compare, or save the `source` field, bringing the data consumer in sync with the data generator.

### 4. Debugging the `bargains_generator`

The `generate --bargains` command was also suffering from several issues.

*   **Progress Output:** I updated the paginated fetcher to use a single, updating line for progress instead of printing a new line for every page.
*   **Infinite Loop:** You reported that the process was fetching millions of prices in an infinite loop. I diagnosed this as a critical bug in the `ExportPricesView` API. A default ordering on the `Price` model (`ordering = ['product__name']`) was conflicting with the `CursorPagination` logic, causing the paginator to get stuck in a loop.
*   **The Fix:** I fixed this by adding `.order_by('id')` to the queryset in `ExportPricesView`, which overrides the problematic default and gives the paginator the stable, unique ordering it requires to function correctly.

### 5. New Data Retention Strategy: Price Deletion

After using a temporary `count_prices` command (which I created and then enhanced with per-store counts at your request) to confirm the database contained over 3.7 million price records, you proposed a new strategy to delete redundant prices.

*   **The Plan:** We planned to delete prices for non-anchor stores at two key moments: after a successful merge and after a successful health check.
*   **Implementation:**
    1.  I modified `intergroup_comparer.py` to delete all prices for all stores from a smaller group after it has been successfully merged into a larger one.
    2.  I modified `internal_group_health_checker.py` to delete all prices for a member store after a health check confirms its prices match its anchor.
*   **Documentation:** Finally, I updated the `grouping_strategy.txt` file to document this new, aggressive data retention policy.

### 6. Group Maintenance Orchestration and Bug Fixes

*   **Decoupling Group Checks:** You requested that group maintenance checks run regardless of whether new product files are processed. I refactored the `update` management command to call the `GroupMaintenanceOrchestrator` directly, moving it out of the `UpdateOrchestrator`'s conditional execution.
*   **`NameError` Fix:** After the refactoring, a `NameError: name 'Price' is not defined` occurred in `internal_group_health_checker.py`. I fixed this by adding the missing `from products.models import Price` import.
*   **`SyntaxError` Fix:** A `SyntaxError: 'continue' not properly in loop` was introduced in `internal_group_health_checker.py` due to an indentation error during a previous `replace` operation. I corrected the indentation to properly place the `continue` statement within its loop.

### 7. Further Performance Optimizations for Group Maintenance

*   **Optimizing Anchor Retrieval:** You identified an inefficiency in `intergroup_comparer.py` where finding anchors with current pricing involved multiple database queries and Python-side processing. I optimized this to a single, more efficient database query using Django ORM's filtering across relationships.
*   **Pre-fetching Prices for Comparisons:** To drastically reduce database queries during price comparisons, I implemented a significant refactoring:
    *   **`PriceComparer` Refactoring:** The `PriceComparer` class was made database-agnostic. Its `compare` method now expects pre-fetched price dictionaries as arguments, removing all internal database query logic.
    *   **`IntergroupComparer` Update:** Modified its `run` method to pre-fetch all prices for all candidate anchors in a single query at the beginning of each pass. These prices are stored in an in-memory cache and passed to the `PriceComparer.compare` method.
    *   **`InternalGroupHealthChecker` Update:** Modified its `run` method to pre-fetch prices for the anchor and all members of a group in a single query before processing that group. These prices are then passed to the `PriceComparer.compare` method.
*   **Health Check Caching:** To avoid redundant health checks, I implemented a new caching mechanism:
    *   **`HealthCheckCacheManager`:** Created a new utility class to manage a `health_check_cache.py` file. This cache stores records of member stores that have been confirmed healthy within the last 7 days.
    *   **`InternalGroupHealthChecker` Integration:** Integrated the `HealthCheckCacheManager` into `InternalGroupHealthChecker`. It now skips health checks for members found in the cache and records newly confirmed healthy members.
*   **Optimizing Health Check Deletion:** The price deletion for healthy members in `internal_group_health_checker.py` was initially one query per member. I optimized this to a single bulk delete query per group, collecting all healthy members' IDs before deletion.
*   **Stale Anchor Handling:** The logic in `internal_group_health_checker.py` was refined. If an anchor's data is stale, the entire group's health check is now skipped, and the anchor is flagged for re-scrape once, avoiding redundant comparisons against a stale anchor.

### 8. Data Freshness Filter for Bargain Generation

*   **Implementation:** To ensure bargains are generated only from fresh data, I implemented a 7-day freshness filter.
    *   **`ExportPricesView` Update:** Modified the API view to accept and filter prices by a `scraped_date_gte` query parameter.
    *   **`BargainsGenerator` Update:** Modified the generator to calculate a 7-day freshness threshold and include it as a `scraped_date_gte` parameter in its API request for prices.