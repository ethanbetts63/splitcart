# Project Overview

This project is a Django-based application called "splitcart". It appears to be a web scraper and data aggregation tool for comparing prices of products from different Australian grocery stores, specifically coles, woolworths, IGA, spudshed and aldi. The project is structured into three main Django apps: `companies`, `products`, and `api`.

**Key Technologies:**

*   **Backend:** Django
*   **Database:** SQLite (as per `settings.py`)
*   **Data Scraping:** The project uses custom scrapers to fetch data from stores websites. 

**Architecture:**

1.  **Data Scraping:**
    *   The `scrape` management command is used to initiate the scraping process, with options for specific stores (e.g., `python manage.py scrape --coles`).
    *   For Woolworths, the scraper dynamically fetches categories from their API, and store data is loaded from a JSON file. For other stores, categories and store data are hardcoded.
    *   Scraped data is saved as raw JSON files in the `api/data/raw_data` directory.

2.  **Data Processing:**
    *   The `process_raw_data` management command processes the raw JSON files.
    *   It combines data by category and archives it in the `api/data/processed_data` directory.

3.  **Database Update:**
    *   The `update_db` management command is used to update the database with processed data, including stores, products, and archived information. It consolidates the functionality of `update_database`, `update_store_database`, and `update_database_from_archive`.

**Models:**

*   **`companies` app:**
    *   `Store`: Represents a grocery store (e.g., coles, woolworths).
    *   `Category`: Represents a product category, with support for hierarchical categories.
*   **`products` app:**
    *   `Product`: Represents a master product, independent of any single store.
    *   `Price`: Represents the price of a product at a specific store on a specific date.

## Management Commands

Here is a brief overview of the available management commands:

*   `find_stores`: Finds stores for a specific company and saves the data to be processed.
*   `scrape`: Scrapes product data from the websites of the specified stores.
*   `process_raw_data`: Processes the raw JSON files generated by the scraper.
*   `update_db`: A consolidated command that updates the database with all new data, including stores, products, and prices.
*   `build_company_jsons`: Generates JSON archives containing data about companies and their stores.
*   `build_store_jsons`: Generates detailed JSON archives for each store, including product and price history.
*   `analyze_data`: A flexible tool for data analysis that can generate various reports, charts, and heatmaps.
*   `compare_stores`: A utility to compare the product offerings between two specified stores.
*   `get_woolworths_substitutes`: A specialized command to fetch substitute product information from Woolworths.

## Data Pipeline Flow

This section outlines the standard workflow for collecting, processing, and archiving data.

### Step 1: Discover and Add Stores

1.  **Find Stores:** Use the `find_stores` command to discover store locations for a specific company.
    ```bash
    python manage.py find_stores --company <company_name>
    ```
2.  **Update Database:** Use the `update_db` command to add the newly discovered stores to the database.
    ```bash
    python manage.py update_db --stores
    ```

### Step 2: Scrape Product Data

*   **Scrape:** Use the `scrape` command to fetch product data for the stores of a specific company.
    ```bash
    python manage.py scrape --<company_name>
    ```

### Step 3: Process Raw Data

*   **Process:** Use the `process_raw_data` command to process the raw JSON files generated during scraping.
    ```bash
    python manage.py process_raw_data
    ```

### Step 4: Update Database with Products

*   **Update:** Use the `update_db` command again, this time to populate the database with the processed product and price information.
    ```bash
    python manage.py update_db --products
    ```

### Step 5: Analyze Data (Optional)

*   **Analyze:** Use the `analyze_data` command to generate reports, charts, and heatmaps to gain insights from the collected data.
    ```bash
    # Example: Generate a product overlap heatmap for all companies
    python manage.py analyze_data --report company_heatmap

    # Example: Generate a store-level heatmap for a specific company
    python manage.py analyze_data --report store_heatmap --company-name "Woolworths"
    ```

### Step 6: Generate Archives

1.  **Company Archives:** Use the `build_company_jsons` command to create company-level data archives.
    ```bash
    python manage.py build_company_jsons
    ```
2.  **Store Archives:** Use the `build_store_jsons` command to create store-level data archives.
    ```bash
    python manage.py build_store_jsons
    ```