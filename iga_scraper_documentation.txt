This document outlines the complete workflow for the IGA scraping system, including the API structure, category system, and the implementation of our scraper.

### 1. IGA API Workflow

The IGA online shopping API is the core of the scraping process. It requires a specific cookie to be set for all requests to function correctly.

**A. Session and Authentication:**

- **Critical Cookie:** The most important part of interacting with the API is setting the store context. All requests to the API must include a cookie named `iga-shop.retailerStoreId`.
- **Value:** The value of this cookie must be the `retailerStoreId` of the specific store you want to scrape (e.g., `"48102"` for Balga IGA).
- **Implementation:** Our scraper (`scrape_and_save_iga.py`) creates a `requests.Session` object and sets this cookie at the beginning of the process. This ensures all subsequent API calls within that session are for the correct store.

**B. API Endpoints:**

There are two main endpoints used in our scraping process:

1.  **Get Category Hierarchy:**
    - **URL:** `https://www.igashop.com.au/api/storefront/stores/{store_id}/categoryHierarchy`
    - **Method:** `GET`
    - **Purpose:** This endpoint retrieves the entire category and subcategory structure for a given store.
    - **Response:** It returns a nested JSON object where each category can contain a `children` array of subcategories.

2.  **Search for Products in a Category:**
    - **URL:** `https://www.igashop.com.au/api/storefront/stores/{store_id}/categories/{category_name}/search`
    - **Method:** `GET`
    - **Purpose:** This endpoint fetches the products within a specific category.
    - **Parameters:**
        - `take`: The number of products to retrieve per page (we use 36).
        - `skip`: The number of products to offset for pagination (e.g., `skip=36` for page 2).
    - **Response:** It returns a JSON object containing an `items` array, where each item is a product.

### 2. Category System

The category system is hierarchical, meaning categories can have subcategories, which can have their own subcategories, and so on.

- **Fetching:** The `get_iga_categories.py` utility is responsible for fetching this data from the `/categoryHierarchy` endpoint.
- **Processing:** The utility then recursively traverses the nested JSON response. It extracts the `name` of every single category and subcategory it finds.
- **Output:** It returns a single, flat list of all category names (e.g., `["Fruit", "Apples", "Pears", "Vegetables", "Leafy Greens"]`). The scraper then iterates through this flattened list to scrape each category one by one.

### 3. Scraper Implementation

The scraping process is managed by a combination of a Django management command and several helper scripts.

**Step 1: Initialization (`manage.py scrape_iga`)**

- **Entry Point:** The process is started by running `python manage.py scrape_iga`.
- **Store List:** The command reads the `api/data/store_data/stores_iga/iga_stores_by_state.json` file. This file, which we recently created from the definitive online store list, is the source of truth for which stores to scrape.
- **Looping:** The command loops through the states and a few stores within each state, passing the `store_name` and `retailerId` to the core scraping function.

**Step 2: Core Scraping (`scrape_and_save_iga.py`)**

- **Session Setup:** It initializes a `requests.Session` and sets the critical `iga-shop.retailerStoreId` cookie.
- **Category Fetching:** It calls `get_iga_categories()` to get the flat list of all categories for the given store.
- **Main Loop:** It iterates through each category name in the list.
- **Pagination Loop:** For each category, it enters a `while` loop to paginate through all the product pages, using the `skip` and `take` parameters in the API call.

**Step 3: Data Handling and Saving**

- **Cleaning:** After fetching a page of products, the raw JSON data is passed to the `clean_raw_data_iga.py` utility. This script extracts only the necessary product information and puts it into our standardized format.
- **Saving:** The cleaned data for *each page* is saved as a separate JSON file in the `api/data/raw_data/` directory. The filename includes the company, store ID, category, and page number.

**Step 4: Checkpointing and Resilience**

The scraper is designed to be resilient to interruptions using a centralized checkpointing system.

- **File:** Progress is stored in `api/data/checkpoints.json`.
- **Process:**
    1.  **`read_checkpoint('iga')`:** When a scrape begins, it reads this file to see if it was in the middle of scraping a specific store.
    2.  **Resuming:** If it finds a checkpoint for the current store and category, it will skip to the `last_completed_page` + 1.
    3.  **`update_page_progress()`:** After each page is successfully downloaded and saved, this function is called to update `checkpoints.json` with the page number that was just completed.
    4.  **`mark_category_complete()`:** When all pages for a category have been scraped, this function is called to add the category to the `completed_categories` list in the checkpoint.
    5.  **`clear_checkpoint('iga')`:** Once all categories for a store are finished, the entire checkpoint for IGA is cleared, ensuring the next run starts fresh on a new store.
