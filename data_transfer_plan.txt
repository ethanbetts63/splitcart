# Data Transfer Architecture Plan: Direct HTTP Transfer Approach

This document outlines the plan for transferring scraped product data (as .jsonl files) from the local `scraping` application to the server-hosted `data_management` application using a direct HTTP connection.

## Core Concept

The `scraping` app will act as a client, sending compressed data directly to a dedicated API endpoint on the `data_management` app, which will act as the server. This approach is self-contained and does not rely on any third-party cloud storage services.

## `data_management` App (Server-side) Workflow

1.  **New API App:**
    *   A new Django app named `api` will be created to handle incoming data transfers.

2.  **Secure API Endpoint:**
    *   A new view, `FileUploadView`, will be created within the `api` app.
    *   This view will be mapped to a URL, such as `/api/upload/`.
    *   The endpoint will be secured using an API key. The server will expect this key to be present in the request headers to authorize the upload.

3.  **File Reception and Processing:**
    *   The `FileUploadView` will be responsible for:
        a.  **Authentication:** Validating the API key from the request header.
        b.  **Receiving Data:** Accepting a compressed `.jsonl.gz` file via an HTTP POST request.
        c.  **Decompression:** Decompressing the received file back into its `.jsonl` format.
        d.  **Moving to Inbox:** Moving the resulting `.jsonl` file into the `data_management/data/product_inbox/` directory.
        e.  **Responding:** Sending a success or failure response back to the client.

4.  **Existing Database Update Process:**
    *   The existing `python manage.py update_db --products` command will work as-is, processing the `.jsonl` files that appear in its `product_inbox`.

## `scraping` App (Local Machine) Workflow

1.  **Outbox Directory:**
    *   The `scraping` app will save its output `.jsonl` files to a directory named `scraping/data/product_outbox/`.

2.  **New Management Command: `upload_data`**
    *   A new management command, `python manage.py upload_data`, will be created in the `scraping` app.
    *   This command will perform the following steps:
        a.  **Configuration:** Read the server's URL and the secret API key from the project settings or an environment file.
        b.  **Iterate:** Scan the `scraping/data/product_outbox/` directory for any `.jsonl` files.
        c.  **Compress:** For each file, create a compressed version using `gzip` (`my_file.jsonl.gz`).
        d.  **Send Data:** Use a library like `requests` to send an HTTP POST request to the server's `/api/upload/` endpoint. The request will include the compressed file and the API key in the headers.
        e.  **Handle Response:** Check the server's response. If the upload was successful, proceed to the next step. If it failed, log the error and decide on a retry strategy.
        f.  **Archive:** After a successful upload, move the original `.jsonl` file from the `product_outbox` to a `scraping/data/archive/` directory.

## Development and Testing

*   This entire workflow can be developed and tested on a single machine by running two instances of the Django development server on different ports, or by running one server instance and using the `upload_data` command in a separate terminal.
*   For production, the `data_management` app must be run with a production-grade web server (e.g., Gunicorn, uWSGI).

## Trade-offs of this Approach

*   **Pros:**
    *   **Self-Contained:** No dependency on external cloud services.
    *   **Direct:** Data flows directly from the scraper to the server.
*   **Cons:**
    *   **Tight Coupling:** The server must be online and accessible for the scraper to be able to upload data.
    *   **Network Resilience:** Requires careful implementation of error handling and retry logic to cope with network interruptions.
    *   **Security:** The security of the file upload endpoint is the developer's responsibility.