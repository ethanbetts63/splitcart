# Data Transfer Architecture Plan: Direct HTTP Transfer Approach

This document outlines the plan for transferring scraped product data (as .jsonl files) from the local `scraping` application to the server-hosted `data_management` application using a direct HTTP connection.

## Core Concept

The `scraping` app will act as a client, sending compressed data directly to a dedicated API endpoint on the `data_management` app, which will act as the server. This approach is self-contained and does not rely on any third-party cloud storage services.

## `data_management` App (Server-side) Workflow

1.  **New API App:**
    *   A new Django app named `api` will be created to handle incoming data transfers.

2.  **Secure API Endpoint:**
    *   A new view, `FileUploadView`, will be created within the `api` app.
    *   This view will be mapped to a URL, such as `/api/upload/`.
    *   The endpoint will be secured using an API key. The server will expect this key to be present in the request headers to authorize the upload.

3.  **File Reception and Processing:**
    *   The `FileUploadView` will be responsible for:
        a.  **Authentication:** Validating the API key from the request header.
        b.  **Receiving Data:** Accepting a compressed `.jsonl.gz` file via an HTTP POST request.
        c.  **Decompression:** Decompressing the received file back into its `.jsonl` format.
        d.  **Moving to Inbox:** Moving the resulting `.jsonl` file into the `data_management/data/product_inbox/` directory.
        e.  **Responding:** Sending a success or failure response back to the client.

4.  **Existing Database Update Process:**
    *   The existing `python manage.py update_db --products` command will work as-is, processing the `.jsonl` files that appear in its `product_inbox`.

## `scraping` App (Local Machine) Workflow

1.  **Outbox Directory:**
    *   The `scraping` app will save its output `.jsonl` files to a directory named `scraping/data/product_outbox/`.

2.  **New Management Command: `upload_data`**
    *   A new management command, `python manage.py upload_data`, will be created in the `scraping` app.
    *   This command will perform the following steps:
        a.  **Configuration:** Read the server's URL and the secret API key from the project settings or an environment file.
        b.  **Iterate:** Scan the `scraping/data/product_outbox/` directory for any `.jsonl` files.
        c.  **Compress:** For each file, create a compressed version using `gzip` (`my_file.jsonl.gz`).
        d.  **Send Data:** Use a library like `requests` to send an HTTP POST request to the server's `/api/upload/` endpoint. The request will include the compressed file and the API key in the headers.
        e.  **Handle Response:** Check the server's response. If the upload was successful, proceed to the next step. If it failed, log the error and decide on a retry strategy.
        f.  **Archive:** After a successful upload, move the original `.jsonl` file from the `product_outbox` to a `scraping/data/archive/` directory.

## Development and Testing

*   This entire workflow can be developed and tested on a single machine by running two instances of the Django development server on different ports, or by running one server instance and using the `upload_data` command in a separate terminal.
*   For production, the `data_management` app must be run with a production-grade web server (e.g., Gunicorn, uWSGI).

## Trade-offs of this Approach

*   **Pros:**
    *   **Self-Contained:** No dependency on external cloud services.
    *   **Direct:** Data flows directly from the scraper to the server.
*   **Cons:**
    *   **Tight Coupling:** The server must be online and accessible for the scraper to be able to upload data.
    *   **Network Resilience:** Requires careful implementation of error handling and retry logic to cope with network interruptions.
    *   **Security:** The security of the file upload endpoint is the developer's responsibility.

## Implementation Status

### Server-Side (`data_management` app)

1.  **New `api` App Created:** A new Django app named `api` has been created at `C:\Users\ethan\coding\splitcart\api`.
2.  **`FileUploadView` Implemented:** The `FileUploadView` has been created in `C:\Users\ethan\coding\splitcart\api\views\file_upload_view.py`. This view handles secure file reception, decompression, and moving files to the inbox.
3.  **URL Configuration:**
    *   `api/urls.py` has been created at `C:\Users\ethan\coding\splitcart\api\urls.py` to map `/upload/` to the `FileUploadView`.
    *   The `api.urls` have been included in the root `splitcart/urls.py` at `C:\Users\ethan\coding\splitcart\splitcart\urls.py`.
4.  **`data_management` App URLs:** An empty `urls.py` has been created for the `data_management` app at `C:\Users\ethan\coding\splitcart\data_management\urls.py` and included in the root `splitcart/urls.py`.
5.  **API Key Configuration:**
    *   `API_SECRET_KEY` and `API_SERVER_URL` settings have been added to `settings.py` (`C:\Users\ethan\coding\splitcart\splitcart\settings.py`).
    *   These settings are now loaded from a `.env` file (`C:\Users\ethan\coding\splitcart\.env`) using `python-dotenv`.

### Client-Side (`scraping` app)

1.  **`upload_data` Management Command:** The `upload_data` command has been implemented in `C:\Users\ethan\coding\splitcart\scraping\management\commands\upload_data.py`. This command handles file compression, sending to the server, and archiving.
2.  **Outbox and Archive Directories:**
    *   The `scraping/data/product_outbox/` directory has been created at `C:\Users\ethan\coding\splitcart\scraping\data\product_outbox`.
    *   The `scraping/data/archive/` directory has been created at `C:\Users\ethan\coding\splitcart\scraping\data\archive`.

## Updates Since Initial Plan

This section details the modifications and improvements made to the data transfer architecture since the initial plan was drafted.

### 1. `upload_data` Command URL Construction Fix
**Issue:** The `upload_data` management command was incorrectly constructing the server upload URL using `os.path.join`. On Windows systems, this resulted in backslashes in the URL (e.g., `/\api/upload/`), leading to a `404 Not Found` error during the HTTP POST request.
**Resolution:** The `upload_url` construction in `scraping/management/commands/upload_data.py` was updated to use string concatenation with `rstrip('/')` to ensure forward slashes and platform-independent URL formation.

### 2. Scraper Output Directory Relocation
**Issue:** Scrapers were initially configured to output `.jsonl` files directly into `scraping/data/product_inbox`. This conflicted with the new workflow where the `product_inbox` is intended for files received by the server, and local scrapers should place files in an outbox for subsequent upload.
**Resolution:** The `JsonlWriter` class in `scraping/utils/product_scraping_utils/jsonl_writer.py` was modified. The `self.inbox_path` variable, which determines the final destination of scraped files, was changed from `scraping/data/product_inbox` to `scraping/data/product_outbox`.

### 3. Archive Directory Renaming
**Issue:** The directory used by the `upload_data` command to store successfully uploaded files was named `archive`. The user requested a more descriptive name for this temporary storage.
**Resolution:** The `archive` directory was renamed to `temp_jsonl_product_storage`. Correspondingly, the `archive_path` variable in `scraping/management/commands/upload_data.py` was updated to reflect this new directory name.