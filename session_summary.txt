# Session Summary

**Date:** August 26, 2025

**Objective:** To design, implement, and debug an automatic brand synonym generation system, and to subsequently build a robust barcode cleaning utility and a product name variation detector to improve data quality.

---

### 1. Automatic Brand Synonym Generation

**Concept:**
We implemented a system to automatically detect potential brand synonyms. The core idea is to leverage product barcodes as a ground truth: when two products from different scrapes share the same barcode but have different brand names, they are flagged as potential synonyms.

**Implementation Details:**
- The main logic was placed in a new utility at `api/utils/synonym_utils/handle_barcode_match.py`.
- This utility is triggered from within the `batch_create_new_products.py` script whenever a barcode match is found.
- Newly discovered synonyms are saved to `api/data/analysis/generated_brand_synonyms.json`.

**Relevant Files:**
- `api/utils/synonym_utils/handle_barcode_match.py`
- `api/utils/synonym_utils/save_synonym.py`
- `api/utils/synonym_utils/load_synonyms.py`
- `api/data/analysis/generated_brand_synonyms.json`

---

### 2. Interactive Synonym Review Tool

**Concept:**
To ensure accuracy and establish correct brand directionality, we integrated the processing of the newly generated synonyms into the existing `create_brand_rules.py` management command.

**Implementation Details:**
- A `--generated` flag was added to the `create_brand_rules` command.
- When run with this flag, the command now reads from `generated_brand_synonyms.json` instead of its default CSV file.

**Relevant Files:**
- `api/management/commands/create_brand_rules.py`
- `api/utils/management_utils/generated_synonym_reader.py`

---

### 3. Product Name Variation Detection

**Concept:**
Building on the brand synonym generation, we implemented a similar system to detect variations in product names. This feature also uses barcode matches as a source of truth to identify different phrasings for the same product across different stores.

**Implementation Details:**
- A new utility, `handle_name_variations`, was created to compare the names of products that share a barcode.
- If the names are different, a record containing the barcode and both name variations is saved to `api/data/analysis/generated_name_variations.json`.
- The logic was also designed to ignore "notfound" barcodes to ensure the quality of the collected data.

**Relevant Files:**
- `api/utils/database_updating_utils/handle_name_variations.py`
- `api/utils/database_updating_utils/save_name_variation.py`
- `api/data/analysis/generated_name_variations.json`

---

### 4. Barcode Cleaning Utility

**Concept:**
During our work, we identified several data quality issues with barcodes. To address this, we created a new "barcode cleaner" utility to standardize and validate barcodes as soon as they are scraped.

**Implementation Details:**
The `clean_barcode` utility, located at `api/utils/scraper_utils/clean_barcode.py`, was created to handle:
- **Paired Barcodes:** Correctly processes fields containing both 12- and 13-digit barcodes.
- **UPC to EAN Conversion:** Automatically converts 12-digit UPCs to 13-digit EANs.
- **Internal Store Codes:** Identifies and removes short, store-specific codes.
- **Invalid Data:** Discards invalid data like "notfound".

**Integration:**
This new utility was integrated into the data cleaning process for all scrapers that provide barcode data (IGA and Woolworths).

**Relevant Files:**
- `api/utils/scraper_utils/clean_barcode.py`
- `api/utils/scraper_utils/clean_raw_data_iga.py`
- `api/utils/scraper_utils/clean_raw_data_woolworths.py`

---

### 5. Advanced Data Reconciliation Strategy (Discussion)

**The "Timing Gap" Problem:**
We identified a critical challenge in the data pipeline: new product name variations are discovered in the database update phase, but the normalized keys used for de-duplication are created back in the scraping phase. This can lead to temporary duplicate products being created if multiple new variations appear in the same batch.

**The "Two-Pass Clean-up" Solution:**
You proposed an elegant and robust solution to this problem:
1.  **Accept Temporary Duplicates:** Allow the main `update_db` process to run, creating potential duplicates but also compiling a list of newly discovered name variations.
2.  **Run a Clean-up Function:** At the end of the `update_db` command, a new, dedicated "clean-up" function will be triggered.
3.  **Targeted Merging:** This function will use the list of newly discovered names to find any temporary duplicates, merge their essential information (like new prices) into the correct canonical product, and then safely delete the duplicate.

This "Two-Pass" approach is the agreed-upon strategy for the future as it favors simplicity, safety, and maintainability over a more complex real-time solution.

---

### 6. Simplifications & Improvements

- **Code Simplification:** We removed a `clean_brand_name` utility and replaced it with a more direct `.lower().strip()` call.
- **Logging Removal:** After debugging, we removed the dedicated logging for synonym generation to keep the code clean.

---

### 7. Additional Thoughts & Future Considerations

- **`store_product_id` Cleaning:** A similar cleaning utility for the `store_product_id` field would be a valuable next step.
- **Review Tools:** An interactive review tool could be built for the new `generated_name_variations.json` file, similar to the one for brands.
- **Scraper Maintenance:** The discovery that Coles and Aldi do not provide barcode data, and that the Spudshed cleaning script is empty, highlights the ongoing need to monitor and maintain the scrapers.



my notes: 
model has a list of alternative names for products. 

then when you go to make the translation table you can create temporary cache? or if you are just making the translation table once every now and a again. You just query every produt and ask what are your alternative names. Then you could build the .py file from that. you would recreate the .py at the end of every database update. 

we can only generate product name variations from the barcode or maybe the sku. So product name variations are discovered in the database updating phase. You discover that the barcode of a product that you are adding into the database already exists. therefor merge. There for alternative name. 

therefor next time when you go to create the translation table you will have additional name variations. 

heres the problem. Theres a gap. Because the normalized strings are created using the name variations in step 1. And the name variations are discovered in step 3. The discovery will only benefit you on the next scraping run. Not immediately. So if you are process the data from multiple stores. You are going to have duplicates sneak into the system immediately. 

as you update the database you could keep a list of discovered name variations. And as you iterate through new incoming products you check if their name is on that list. If it is you need to recreate their normalized string so that the lack of uniqueness will be detected? 



the problem is that if we rerun the create normalized string command mid database update it won't yet have access to the new discovered names. and we can't regenerate the whole product name translation file and read it into memory every time we find a name variation because it would take to long. 

alternative idea. Run a clean up function. You have a list of discovered names. all you do at the end of the database updating is you check the database for those names. If you find a product with that name you know you need to merge it with an existing product. 


1. product_name_translation_table.py is created at the end of the previous database update by reading the name variations field of every product in the database. 
2. the translation table is used to clean the product name used in the normalized name brand size string for each scraped product. 
3. database updating: We check the barcode. and find a match but with a new name that is not in the name variation field for that product. We update the field to add the new name. and we also add the new name to a in memory list called discovered names. 
4. now we continue as usual slowly growing our list of discovered names. We will accept here that some duplicates will get through because our normalized strings were created before the discovered names list. 
5. when the database update is finsised we take our discovered names list. Look for products with those names and if they exist we merge them with the product that already exists with that alterative name. 
6. Then finally we recreate the product_name_translation_table.py to make it current for next time. 

there is still a gap here. And thats that the scrapers are normally running at the same time as the database updater. THeres two problems here. One is that the store being scraper during the database update will be using an outdated product_name_translation_table and because they will be likely put into the database by a separate call of update database they also wont have access to the disovered names list. So duplicates will sneak through. To make this worse. The scrapers often run for hours at a time. So im not sure if the version of product_name_translation_table will ever be updated in memory even if the physical file is updated. 

so then what do you do? here are some ideas: 
- a smarter cleaning function. Maybe something that can keep track of the products that have potential to slip through the cracks. Or just a more clever way to clean more thouroughly. 
- update the database immediately instead of using an inbox system with json files. I don't like this because it would be a massive change and i like having the inbox because it seperates things well. 
- we could move generating the normalized name function to the database stage. I think we would still potentially have a gap but it would be solvable with a cleanup function. I like the way we are doing it currently by generating the name at the start because it immediately removes a lot of duplicates from the files in the inbox. Which not only saves on space but also reduces the load on the database update. 
- maybe something smarter than a .py file for product_name_translation_table but i dont think that solves the core issue. 

right now im leaning towards figuring out smarter cleaning. I just don't know what that looks like yet. Maybe we could keep a list of discovered name variations outside the cleanup function in a json file or something. We could let the entries in that stay for a few days before being cleaned up to guarantee that there are no duplicates slipping through. 


