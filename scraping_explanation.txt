# Scraping Process Explanation

This document explains the data scraping process in the splitcart project.

## Overview

The scraping process is initiated by the `scrape` management command, which deploys company-specific scrapers to fetch product data from different stores. The scraped data is saved to `.jsonl` files in the `product_inbox` directory, ready for database processing.

## Core Components

### 1. `scrape` Management Command

- **Purpose:** The main entry point for the scraping process.
- **Functionality:**
    - Takes command-line arguments (e.g., `--woolworths`, `--coles`) to specify which company to scrape. Runs all if none are specified.
    - Fetches a batch of active stores for the specified company, prioritizing those that haven't been scraped recently.
    - Instantiates and runs the appropriate scraper for each store.
    - Updates the `last_scraped_products` timestamp for the store after a successful scrape.

### 2. `BaseScraper` Abstract Class

- **Purpose:** Provides a common structure and workflow for all company-specific scrapers.
- **Key Methods:**
    - `run()`: Orchestrates the scraping process by calling the other methods in a predefined order.
    - `setup()`: For initial setup (e.g., creating a `requests.Session`).
    - `get_work_items()`: To get the list of categories or pages to scrape.
    - `fetch_data_for_item()`: To fetch the raw data for a single category/page.
    - `clean_raw_data()`: To clean and structure the raw data.
- **Data Handling:** Uses a `JsonlWriter` to write data to a temporary file, which is moved to the `product_inbox` only upon successful completion.

### 3. Scraper Implementations

- **`WoolworthsScraper`:**
    - Inherits from `BaseScraper`.
    - It is API-based, directly calling the Woolworths API to get product data in JSON format.

- **`scrape_and_save_coles_data` (Coles Scraper):
    - **Inconsistency:** This scraper is implemented as a function, not a class inheriting from `BaseScraper`.
    - **Hybrid Approach:** It uses Selenium to open a browser and obtain session cookies (requiring manual CAPTCHA solving), then uses a `requests` session for the actual scraping.
    - **HTML Scraping:** It extracts data from a JSON object embedded in the HTML of the category pages.
    - **Enrichment:** It has a unique post-scraping step to enrich the data with barcode information.

## Scraping Workflow Summary

1.  The `scrape` command is executed for a specific company.
2.  A batch of stores is selected for scraping.
3.  For each store, the corresponding scraper is run.
4.  The scraper gets a list of categories/pages to work on.
5.  It iterates through them, fetching the raw product data.
6.  The data is cleaned and normalized.
7.  The cleaned data is written to a temporary `.jsonl` file.
8.  On success, the file is moved to the `product_inbox`.
9.  The store's `last_scraped_products` timestamp is updated.

## Potential Improvements

- **Refactor Coles Scraper:** The Coles scraper should be refactored into a class that inherits from `BaseScraper` to ensure consistency and better maintainability.
- **Automate Coles CAPTCHA:** The manual CAPTCHA solving required for the Coles scraper is a major bottleneck. Investigating automated solutions would significantly improve the scraping process.
