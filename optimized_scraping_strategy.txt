# Optimized, Phased Scraping Strategy

## 1. High-Level Goal

To evolve from a brute-force scraping model (scraping all products from all stores, all the time) to a targeted, data-driven model. The aim is to significantly reduce redundant scraping and operational costs while maintaining high data accuracy and responsiveness through a dynamic, intelligent queuing system.

## 2. The Two-Phase Approach

A "cold start" problem exists: we cannot optimize scraping without a complete, time-synchronized dataset to base our analysis on. Therefore, the strategy for each company must be executed in two distinct phases.

### Phase 1: Discovery & Baseline

*   **Objective:** Achieve 100% scrape coverage of all stores for a given company to build a foundational dataset.
*   **Execution:** In this phase, the `scrape` command will operate in a simple "brute-force" mode, prioritizing stores that have never been scraped or have the oldest scrape dates.
*   **Critical Constraint:** This phase is essential for calculating the "True Overlap" metric, as it requires comparing prices from different stores that were scraped on the same day (or as close as possible).

### Phase 2: Optimized & Dynamic Scraping

*   **Objective:** Switch to an intelligent, efficient scraping model based on the analysis of the baseline data.
*   **Execution:** Once the baseline is established and the initial groups are formed, the `scrape` command's logic changes to the "Intelligent Queue" model. This is the ongoing operational state.

## 3. Core Concepts of the Optimized Model

### Store Grouping & The "True Overlap" Metric
The core of the system is grouping stores that are effective "price-twins."
*   **The Metric:** The primary metric for grouping will be **"True Overlap"**: the percentage of shared products between two stores that have the *exact same price*.
*   **The Constraint:** This comparison must be performed on data scraped on the same day to be valid. A price difference could be due to a genuine store-level pricing difference or simply because one store's data is a week older than the other's.
*   **Threshold:** We will start with a high threshold (e.g., >95% True Overlap) to create confident initial groups. This can be tuned over time.

### The Intelligent Queue
In Phase 2, the `scrape` command will no longer scrape the oldest stores. Instead, it will build a fresh, high-priority queue for each run, consisting of:
1.  **Outliers:** Stores that do not fit into any group. These are scraped frequently to keep their unique data fresh and to see if they can be reclassified.
2.  **Group Ambassadors (Rotating):** For each defined group, one store is chosen as the "ambassador" for that scrape run. Its data is used to infer prices for all other stores in the group. Crucially, the ambassador role will be *rotated* to a different store in the group on the next run, providing distributed validation over time.

### Granularity: Category-Level Inference
Even stores that don't qualify for a full-store group may have identical pricing for specific categories.
*   **Strategy:** We can create groups at the category level (e.g., a "Woolworths Pantry QLD" group).
*   **Benefit:** This allows for a more granular optimization. A scrape of a single store could provide "direct" data for some of its categories and "inferred" data for many other stores that share its category-level pricing.

### Data Source Tracking
To maintain data integrity and enable the feedback loop, every price in the database must be marked with its origin.
*   **Implementation:** The `products.Price` model must be modified to include a `source` field.
*   **Values:** This field will have states such as `'direct_scrape'` or `'inferred_group'`.
*   **Purpose:** This allows us to measure the accuracy of our inferences and is the primary trigger for the re-grouping logic. When a direct scrape contradicts a previous inference, the system knows that the group's stability needs to be re-evaluated.

## 4. Rollout & Execution

### Phased Geographic Rollout ("One City at a Time")
To become operational in at least one region sooner, the entire Discovery/Optimization process can be focused on one geographic area at a time (e.g., Brisbane). Learnings from the first city's grouping patterns can then be used to accelerate the process for the next city. This is feasible as we have latitude/longitude data for most stores.

### Dynamic Re-classification
The system must be dynamic. The "True Overlap" scores and group memberships must be re-calculated periodically to adapt to changes in store strategies. The comparison of direct vs. inferred data serves as the primary trigger for this re-evaluation.

## 5. Open Questions & Key Decisions

This strategy opens up several questions that will need to be answered during implementation:

1.  **Thresholds:** What is the initial "True Overlap" percentage required to form a group (95%, 98%)? What level of price deviation in a spot-check triggers a full re-scrape?
2.  **Re-analysis Frequency:** How often should the full, system-wide re-analysis of all groups be performed (e.g., weekly, monthly, quarterly)?
3.  **Ambassador Selection:** When rotating an ambassador, is the selection random, or round-robin?
4.  **Canary Products:** If we implement "micro-scrapes," how many "canary" products are sufficient for a reliable spot check? How are these products selected?
5.  **Geographic Boundaries:** How do we define the geographic boundaries for the "one city at a time" rollout (e.g., by postcode radius, council areas)?
6.  **Data Freshness:** What is the maximum acceptable age for data used in the "True Overlap" analysis? (e.g., must be from the same 24-hour period).