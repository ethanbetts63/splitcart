# Optimized, Phased Scraping Strategy

## 1. High-Level Goal

To evolve from a brute-force scraping model (scraping all products from all stores, all the time) to a targeted, data-driven model. The aim is to significantly reduce redundant scraping and operational costs while maintaining high data accuracy and responsiveness through a dynamic, intelligent queuing system.

## 2. The Two-Phase Approach

A "cold start" problem exists: we cannot optimize scraping without a complete, time-synchronized dataset to base our analysis on. Therefore, the strategy for each company must be executed in two distinct phases.

### Phase 1: Discovery & Baseline

*   **Objective:** Achieve 100% scrape coverage of all stores to build a foundational dataset for the Phase 2 analysis.

*   **Execution: The 80/20 Strategy:** The discovery process will be guided by a blended strategy to ensure both focused completion of major areas and broad, exploratory coverage.
    1.  The system first identifies the largest geographic cluster (by store count) that contains unscraped stores. This becomes the "Primary Target Cluster".
    2.  The scraping process then follows an 80/20 rule: for every 5 stores scraped, 4 will be from the primary target and 1 will be from outside of it.
        *   **80% of Scrapes:** The system selects an unscraped store from within the "Primary Target Cluster". This provides a focused effort to rapidly achieve data completeness for a major geographic area.
        *   **20% of Scrapes:** The system selects a random, unscraped store from anywhere outside the primary cluster. This includes smaller clusters and regional outliers, ensuring the entire dataset makes steady progress.
    3.  Once all stores in the "Primary Target Cluster" have been scraped, the system designates the next largest unscraped cluster as the new primary target, and the process repeats.
    4.  This phase is complete when there are no unscraped stores remaining.

*   **Critical Constraint:** The goal of this phase is to produce a "time-synchronized" dataset, which is essential for the "True Overlap" analysis. This means scraping all stores in a given region as quickly as possible.

### Phase 2: Optimized & Dynamic Scraping

*   **Objective:** Switch to an intelligent, efficient scraping model based on the analysis of the baseline data.
*   **Execution:** Once the baseline is established and the initial groups are formed, the `scrape` command's logic changes to the "Intelligent Queue" model. This is the ongoing operational state.

## 3. Core Concepts of the Optimized Model

### The "Top-Down" Grouping Philosophy with Geographic Seeding
To start the process, we will adopt a "top-down" approach using a single, reliable heuristic: geography.

*   **Geographic Seeding:** We will not guess store sub-types from names. Instead, we will begin by creating initial "monolith" groups based purely on geographic location (e.g., the Perth metropolitan area). This means we start Phase 2 with seed groups like `[Woolworths-Perth]` and `[Coles-Perth]`.

*   **Discovering Sub-Types:** The true store sub-types (e.g., "Metro", "Supermarket") will be *discovered* by the system. As the Two-Ambassador health checks run, the `[Woolworths-Perth]` group will naturally be "chiseled" into smaller, more accurate sub-groups based on the actual pricing behavior of its member stores. This is a more robust, data-driven method than relying on uncertain heuristics.

### The "True Overlap" Metric
*   **The Metric:** The primary metric for grouping will be **"True Overlap"**: the percentage of shared products between two stores that have the *exact same price*.
*   **The Constraint:** This comparison must be performed on data scraped on the same day (or within a tight 48-hour window) to be valid. This is to ensure we are comparing real pricing strategy, not just stale data.

### Health Checks: The Two-Ambassador System
Instead of scraping a single ambassador, the "Intelligent Queue" will select **two** rotating ambassadors from each group for each run. This creates a system of constant, paired re-validation.
*   **If they match:** The group is considered `[Healthy]`. The data from one can be used to infer prices for the rest of the group.
*   **If they don't match:** This is a **Divergence Event**, which triggers the handling process below.

### Handling Divergence (Status Flag Method)
When a two-ambassador health check fails, the decoupled `update_db` and `ScrapeScheduler` processes will communicate indirectly via the database.
1.  **Detection (`update_db`):** After processing the files for ambassadors A and B and finding a mismatch, the `update_db` process will "stamp" their parent `StoreGroup`. It sets the group's `status` to `DIVERGENCE_DETECTED` and populates the `tie_breaker_store` field with a third store (C) from the group.
2.  **Prioritization (`ScrapeScheduler`):** On its next run, the scheduler's highest priority is to query for any groups with the `DIVERGENCE_DETECTED` status. It will read the `tie_breaker_store` for each and place these stores at the top of the scrape queue.
3.  **Resolution (`update_db`):** After the tie-breaker store C is scraped, the `update_db` process will detect it's resolving a divergence. It will perform the 3-way comparison, eject the rogue store by deleting its `StoreGroupMembership`, and finally set the `StoreGroup` status back to `HEALTHY`, clearing the `tie_breaker_store` field.

### Granularity: Category-Level Inference
Even stores that don't qualify for a full-store group may have identical pricing for specific categories.
*   **Strategy:** We can create groups at the category level (e.g., a "Woolworths Pantry QLD" group).
*   **Benefit:** This allows for a more granular optimization. A scrape of a single store could provide "direct" data for some of its categories and "inferred" data for many other stores that share its category-level pricing.

### Data Source Tracking
To maintain data integrity and enable the feedback loop, every price in the database must be marked with its origin.
*   **Implementation:** The `products.Price` model must be modified to include a `source` field.
*   **Values:** This field will have states such as `'direct_scrape'` or `'inferred_group'`.
*   **Purpose:** This allows us to measure the accuracy of our inferences and is the primary trigger for the re-grouping logic.

## 4. Rollout & Execution

### Geographic Clustering Algorithm
A command, `cluster_stores`, has been implemented to serve as the foundation for the geographic rollout. This command uses the DBSCAN algorithm to process all stores for each company and automatically group them into distinct geographic clusters based on their location density. It saves these groups to the database, identifying isolated regional stores as outliers. A corresponding `visualize_clusters` command has also been created to generate map images of these results, providing a visual way to verify the clusters. This completes the prerequisite for the phased rollout.

### Initial Rollout Strategy: The Perth Pilot
Once the clustering algorithm is built, the initial rollout will be geographically focused.

*   **Pilot Cluster:** Perth, Western Australia.
*   **Execution:**
    1.  Run the geographic clustering algorithm to identify the "Perth" cluster of stores.
    2.  All stores within this cluster will constitute the cohort for the **Phase 1: Discovery** scrape.
    3.  Once the Perth discovery is complete and its data is being managed by the **Phase 2: Optimization** model, the learnings can be applied to the next target cluster (e.g., Adelaide).

### Proposed Data Model
To support the grouping logic, the database will be extended with two new models:
*   **`StoreGroup`:** Represents a group itself. It will have a name, a link to its parent company, and an `is_active` flag. To manage divergence events, it will also have:
    *   `status`: A field to track the group's health (e.g., `HEALTHY`, `DIVERGENCE_DETECTED`).
    *   `tie_breaker_store`: A nullable link to a `Store` that is designated for a priority scrape to resolve a divergence.
*   **`StoreGroupMembership`:** A linking table that connects a `Store` to a `StoreGroup`. This will hold metadata about the membership, such as a `last_scraped_as_ambassador` timestamp. Outliers will be stores with no entry in this table.

### Dynamic Re-classification
The system must be dynamic. The "True Overlap" scores and group memberships must be re-calculated periodically to adapt to changes in store strategies. The comparison of direct vs. inferred data serves as the primary trigger for this re-evaluation.

### Scrape Scheduler Implementation
The core intelligence of the scraping process will be encapsulated in a dedicated `ScrapeScheduler` class. This class will be responsible for generating the queue of stores to be scraped in any given run. It will have two primary methods, corresponding to the two phases of the project:

*   **`get_discovery_mode_queue()`:** This method, which has already been designed, handles the initial discovery phase. It implements the 80/20 strategy by building a queue that is 80% focused on completing the largest unscraped geographic cluster and 20% focused on randomly exploring all other unscraped stores and outliers.

*   **`get_optimization_mode_queue()`:** This method will be developed for Phase 2. It will generate the daily scrape queue with a different set of priorities:
    1.  **Resolving Divergence:** First, it will queue any "tie-breaker" stores needed to resolve ambiguity in groups that have failed a health check.
    2.  **Maintaining Outliers:** Next, it will queue the outlier store that has the most outdated data.
    3.  **Routine Health Checks:** Finally, it will queue two rotating ambassadors from the healthy group that has gone the longest without a check.

## 5. Open Questions & Key Decisions

This strategy opens up several questions that will need to be answered during implementation:

1.  **Thresholds:** What is the initial "True Overlap" percentage required to form a group (95%, 98%)? What level of price deviation in a spot-check triggers a full re-scrape?
2.  **Re-analysis Frequency:** How often should the full, system-wide re-analysis of all groups be performed (e.g., weekly, monthly, quarterly)?
3.  **Ambassador Selection:** When rotating an ambassador, is the selection random, or round-robin?
4.  **Canary Products:** If we implement "micro-scrapes," how many "canary" products are sufficient for a reliable spot check? How are these products selected?
5.  **Geographic Boundaries:** How do we define the geographic boundaries for the "one city at a time" rollout (e.g., by postcode radius, council areas)?
6.  **Data Freshness:** What is the maximum acceptable age for data used in the "True Overlap" analysis? (e.g., must be from the same 24-hour period).
7. for woolworths/aldi and coles we actually can actually start by having all stores in the same group. then the process of discovery isnt the process of adding to the group its the process of intelligently removing and regrouping. 