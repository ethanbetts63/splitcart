# Optimized, Phased Scraping Strategy

## 1. High-Level Goal

To evolve from a brute-force scraping model (scraping all products from all stores, all the time) to a targeted, data-driven model. The aim is to significantly reduce redundant scraping and operational costs while maintaining high data accuracy and responsiveness through a dynamic, intelligent queuing system.

## 2. The Two-Phase Approach

A "cold start" problem exists: we cannot optimize scraping without a complete, time-synchronized dataset to base our analysis on. Therefore, the strategy for each company must be executed in two distinct phases.

### Phase 1: Discovery & Baseline

*   **Objective:** Achieve 100% scrape coverage of all stores for a given company to build a foundational dataset.
*   **Execution:** In this phase, the `scrape` command will operate in a simple "brute-force" mode, prioritizing stores that have never been scraped or have the oldest scrape dates.
*   **Critical Constraint:** This phase is essential for calculating the "True Overlap" metric, as it requires comparing prices from different stores that were scraped on the same day (or as close as possible).

### Phase 2: Optimized & Dynamic Scraping

*   **Objective:** Switch to an intelligent, efficient scraping model based on the analysis of the baseline data.
*   **Execution:** Once the baseline is established and the initial groups are formed, the `scrape` command's logic changes to the "Intelligent Queue" model. This is the ongoing operational state.

## 3. Core Concepts of the Optimized Model

### The "Top-Down" Grouping Philosophy with Heuristic Seeding
Instead of starting with a single, national "monolith" group for each company, we will adopt a more intelligent "top-down" approach using logical heuristics to form more plausible initial groups.

*   **Heuristic Seeding:** Before any analysis, we will pre-group stores based on data we already have. The primary heuristics will be:
    1.  **Geography:** As defined by our pilot program (e.g., the Perth metropolitan area).
    2.  **Store Sub-Type:** Using clues in the store name or its designated division (e.g., "Metro", "Supermarket", "CBD").

*   **Smarter Starting Point:** This means instead of one giant `[Woolworths-National]` group, we begin Phase 2 with a set of much stronger seed groups, such as `[Woolworths-Perth-Metro]` or `[Coles-Perth-Supermarket]`.

*   **The "Chiseling" Process:** The core strategy of using the Two-Ambassador system to find outliers and split groups still applies, but it now operates on these much more coherent initial blocks. This will dramatically increase efficiency and reduce the number of "schisms" encountered, as the initial assumptions are far more likely to be correct.

### The "True Overlap" Metric
*   **The Metric:** The primary metric for grouping will be **"True Overlap"**: the percentage of shared products between two stores that have the *exact same price*.
*   **The Constraint:** This comparison must be performed on data scraped on the same day (or within a tight 48-hour window) to be valid. This is to ensure we are comparing real pricing strategy, not just stale data.

### Health Checks: The Two-Ambassador System
Instead of scraping a single ambassador, the "Intelligent Queue" will select **two** rotating ambassadors from each group for each run. This creates a system of constant, paired re-validation.
*   **If they match:** The group is considered `[Healthy]`. The data from one can be used to infer prices for the rest of the group.
*   **If they don't match:** This is a **Divergence Event**, which triggers the handling process below.

### Handling Divergence (Flag-less Method)
When a divergence event occurs between ambassadors A and B, we will follow a simple, flag-less rule based on the principle: **"it's better to be outdated than completely wrong."**
1.  **Immediate Action:** Stores A and B are both immediately re-classified as temporary **outliers**. This isolates them from the main group and ensures they are scraped individually until their status is resolved.
2.  **Main Group Status:** The rest of the group (C, D, E...) becomes "headless." No new price inferences are made for these stores. Their existing data is allowed to age gracefully.
3.  **Resolution:** A third tie-breaker store from the group, C, is placed into a high-priority queue.
4.  **Re-integration:** Once C is scraped, it is compared to the fresh data from A. If they match, store A is re-admitted to the group, which is now healthy again. Store B is confirmed as a true, permanent outlier and is not re-admitted.

### Granularity: Category-Level Inference
Even stores that don't qualify for a full-store group may have identical pricing for specific categories.
*   **Strategy:** We can create groups at the category level (e.g., a "Woolworths Pantry QLD" group).
*   **Benefit:** This allows for a more granular optimization. A scrape of a single store could provide "direct" data for some of its categories and "inferred" data for many other stores that share its category-level pricing.

### Data Source Tracking
To maintain data integrity and enable the feedback loop, every price in the database must be marked with its origin.
*   **Implementation:** The `products.Price` model must be modified to include a `source` field.
*   **Values:** This field will have states such as `'direct_scrape'` or `'inferred_group'`.
*   **Purpose:** This allows us to measure the accuracy of our inferences and is the primary trigger for the re-grouping logic.

## 4. Rollout & Execution

### Initial Rollout Strategy: The Perth Pilot
To solve the "cold start" problem, the initial rollout will be geographically focused. We will begin with a pilot program in a single city to build our baseline dataset in a controlled environment.

*   **Pilot City:** Perth, Western Australia.
*   **Execution:**
    1.  Establish the central geographic coordinates for Perth.
    2.  Define a radius (e.g., 50km) to create a "Perth metropolitan area" boundary.
    3.  All stores from all companies that fall within this geographic radius will constitute the entire cohort for the **Phase 1: Discovery** scrape.
    4.  Once the Perth discovery is complete and its data is being managed by the **Phase 2: Optimization** model, the learnings can be applied to the next target city (e.g., Adelaide, Brisbane).

### Dynamic Re-classification
The system must be dynamic. The "True Overlap" scores and group memberships must be re-calculated periodically to adapt to changes in store strategies. The comparison of direct vs. inferred data serves as the primary trigger for this re-evaluation.

## 5. Open Questions & Key Decisions

This strategy opens up several questions that will need to be answered during implementation:

1.  **Thresholds:** What is the initial "True Overlap" percentage required to form a group (95%, 98%)? What level of price deviation in a spot-check triggers a full re-scrape?
2.  **Re-analysis Frequency:** How often should the full, system-wide re-analysis of all groups be performed (e.g., weekly, monthly, quarterly)?
3.  **Ambassador Selection:** When rotating an ambassador, is the selection random, or round-robin?
4.  **Canary Products:** If we implement "micro-scrapes," how many "canary" products are sufficient for a reliable spot check? How are these products selected?
5.  **Geographic Boundaries:** How do we define the geographic boundaries for the "one city at a time" rollout (e.g., by postcode radius, council areas)?
6.  **Data Freshness:** What is the maximum acceptable age for data used in the "True Overlap" analysis? (e.g., must be from the same 24-hour period).
7. for woolworths/aldi and coles we actually can actually start by having all stores in the same group. then the process of discovery isnt the process of adding to the group its the process of intelligently removing and regrouping. 