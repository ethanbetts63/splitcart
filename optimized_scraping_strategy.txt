# Optimized, Phased Scraping Strategy

## 1. High-Level Goal

To evolve from a brute-force scraping model (scraping all products from all stores, all the time) to a targeted, data-driven model. The aim is to significantly reduce redundant scraping and operational costs while maintaining high data accuracy and responsiveness through a dynamic, intelligent queuing system.

## 2. The Two-Phase Approach

A "cold start" problem exists: we cannot optimize scraping without a complete, time-synchronized dataset to base our analysis on. Therefore, the strategy for each company must be executed in two distinct phases.

### Phase 1: Discovery & Baseline

*   **Objective:** Achieve 100% scrape coverage of all stores to build a foundational dataset for the Phase 2 analysis.

*   **Execution: The 80/20 Strategy:** The discovery process will be guided by a blended strategy to ensure both focused completion of major areas and broad, exploratory coverage.
    1.  The system first identifies the largest geographic cluster (by store count) that contains unscraped stores. This becomes the "Primary Target Cluster".
    2.  The scraping process then follows an 80/20 rule: for every 5 stores scraped, 4 will be from the primary target and 1 will be from outside of it.
        *   **80% of Scrapes:** The system selects an unscraped store from within the "Primary Target Cluster". This provides a focused effort to rapidly achieve data completeness for a major geographic area.
        *   **20% of Scrapes:** The system selects a random, unscraped store from anywhere outside the primary cluster. This includes smaller clusters and regional outliers, ensuring the entire dataset makes steady progress.
    3.  Once all stores in the "Primary Target Cluster" have been scraped, the system designates the next largest unscraped cluster as the new primary target, and the process repeats.
    4.  This phase is complete when there are no unscraped stores remaining.

*   **Critical Constraint:** The goal of this phase is to produce a "time-synchronized" dataset, which is essential for the "True Overlap" analysis. This means scraping all stores in a given region as quickly as possible. "time-synchronized"being is a key challenge is there is just simply a limitation on how many stores can be scraped in a short enough period that you can call the data "time-synchronized".

### Phase 2: Optimized & Dynamic Scraping

*   **Objective:** Switch to an intelligent, efficient scraping model based on the analysis of the baseline data.
*   **Execution:** Once the baseline is established and the initial groups are formed, the `scrape` command's logic changes to the "Intelligent Queue" model. This is the ongoing operational state.

## 3. Core Concepts of the Optimized Model

### Key Terminology
To ensure clarity, we will use the following precise terms:
*   **True Overlap:** The primary metric for grouping. It is the percentage of shared products between two stores that have the exact same price, based on data scraped within the same tight time window.
*   **The Ambassador:** For each group, one store is designated the official Ambassador. It serves as the single "source of truth" for the group's current pricing. This is a rolling title, passed to the most recently confirmed matching store.
*   **The Candidate:** A store selected by the `ScrapeScheduler` for a routine health check. Its data is tested against the group's current Ambassador.
*   **The Rogue:** A store that was previously a member of a group but is found to no longer match the Ambassador's pricing. It is ejected from the group and becomes an outlier.

### The "Top-Down" Grouping Philosophy with Geographic Seeding
To start the process, we will adopt a "top-down" approach using a single, reliable heuristic: geography.

*   **Geographic Seeding:** We will not guess store sub-types from names. Instead, we will begin by creating initial "monolith" groups based purely on geographic location (e.g., the Perth metropolitan area). This means we start Phase 2 with seed groups like `[Woolworths-Perth]` and `[Coles-Perth]`.

*   **Discovering Sub-Types:** The true store sub-types (e.g., "Metro", "Supermarket") will be *discovered* by the system. As the Two-Ambassador health checks run, the `[Woolworths-Perth]` group will naturally be "chiseled" into smaller, more accurate sub-groups based on the actual pricing behavior of its member stores. This is a more robust, data-driven method than relying on uncertain heuristics.

### The "True Overlap" Metric
*   **The Metric:** The primary metric for grouping will be **"True Overlap"**: the percentage of shared products between two stores that have the *exact same price*.
*   **The Constraint:** This comparison must be performed on data scraped on the same day (or within a tight 48-hour window) to be valid. This is to ensure we are comparing real pricing strategy, not just stale data.

### Group Maintenance: The Ambassador & Candidate Workflow
This new model provides a clear and robust process for maintaining group integrity. The `ScrapeScheduler` will still scrape pairs of `Candidates` for each health check, but the `update_db` process now follows this logic:

1.  **Fetch Data:** The grouper receives the data for one or more new `Candidate` stores (e.g., C1 and C2) from a specific group. It also fetches the group's current, official `Ambassador` (A) by reading the `ambassador` link on the `StoreGroup` model.

2.  **Perform Check:** It compares the first Candidate (C1) to the Ambassador (A) using the "True Overlap" metric.

3.  **Handle Success:** If C1 is a match with A, the group is healthy. C1's data is used to infer new prices for all other members of the group. The "torch is passed": the parent `StoreGroup`'s `ambassador` field is updated to point directly to the Candidate store C1. The process is complete.

4.  **Handle Simple Divergence (Fallback):** If C1 does *not* match A, the grouper uses the second Candidate (C2) as an immediate fallback. It compares C2 to the Ambassador A.
    *   If C2 matches A, the group is still healthy. C1 is identified as a `Rogue` and is outcast from the group (its `StoreGroupMembership` is deleted). C2's data is used for inference, and C2 becomes the new `Ambassador` by updating the group's `ambassador` field.

5.  **Handle Major Divergence (Schism):** If C1 does not match A, and C2 *also* does not match A, this signals a major problem. The Ambassador itself may be out of date, or a larger schism may have occurred. In this case, the `update_db` process flags the entire `StoreGroup` by setting its `is_active` flag to `False`. This effectively dissolves the group, turning all its members into individual outliers. These outliers will then be re-scraped individually by the scheduler and will organically form new, smaller, stable groups over time.

### The Grouper Workflow (`update_db`)
To keep the system's state clean and explicit, the `update_db` process will use the database to manage its list of stores to check.
1.  **Identify & Stamp Candidates:** At the beginning of a run, the process identifies all stores with new `.jsonl` files. For each of these stores, it adds the `Store` object to the `candidates` relationship on its parent `StoreGroup`. This "stamps" them as the work-list for this run.
2.  **Process & Check:** The grouper logic then queries for groups that have pending candidates and performs the "Ambassador & Candidate Workflow" described above.
3.  **Cleanup:** At the end of the run, the process clears the `candidates` relationship for all the groups it has processed, ensuring the system is clean for the next run.

### Outcasting & Re-homing Rogue Stores
When a `Rogue` store is identified and outcast from its group, it is not immediately classified as a permanent outlier. The `update_db` process will first attempt to find a new home for it. This "re-homing" process must be efficient.

*   **Two-Stage Check:** To avoid computationally expensive comparisons, we will use a two-stage check when comparing the Rogue store against the Ambassadors of other potential groups.
    1.  **Stage 1 (Cheap Test): Range Overlap.** First, the system performs a fast comparison of the two stores' product catalogs (a list of product IDs).
    2.  **Stage 2 (Expensive Test): True Overlap.** If, and only if, the Range Overlap is above a high threshold (e.g., 95%), the system will proceed with the full, computationally expensive "True Overlap" analysis to compare prices.
*   **Final Status:** If the Rogue finds a new matching group, it is moved. If it cannot find a new home after being tested against all viable groups, it is then officially classified as a permanent outlier.

### Granularity: Category-Level Inference
Even stores that don't qualify for a full-store group may have identical pricing for specific categories.
*   **Strategy:** We can create groups at the category level (e.g., a "Woolworths Pantry QLD" group).
*   **Benefit:** This allows for a more granular optimization. A scrape of a single store could provide "direct" data for some of its categories and "inferred" data for many other stores that share its category-level pricing.

### Data Source Tracking
To maintain data integrity and enable the feedback loop, every price in the database must be marked with its origin.
*   **Implementation:** The `products.Price` model must be modified to include a `source` field.
*   **Values:** This field will have states such as `'direct_scrape'` or `'inferred_group'`.
*   **Purpose:** This allows us to measure the accuracy of our inferences and is the primary trigger for the re-grouping logic.

## 4. Rollout & Execution

### Geographic Clustering Algorithm
A command, `cluster_stores`, has been implemented to serve as the foundation for the geographic rollout. This command uses the DBSCAN algorithm to process all stores for each company and automatically group them into distinct geographic clusters based on their location density. It saves these groups to the database, identifying isolated regional stores as outliers. A corresponding `visualize_clusters` command has also been created to generate map images of these results, providing a visual way to verify the clusters. This completes the prerequisite for the phased rollout.

### Initial Rollout Strategy: The Perth Pilot
Once the clustering algorithm is built, the initial rollout will be geographically focused.

*   **Pilot Cluster:** Perth, Western Australia.
*   **Execution:**
    1.  Run the geographic clustering algorithm to identify the "Perth" cluster of stores.
    2.  All stores within this cluster will constitute the cohort for the **Phase 1: Discovery** scrape.
    3.  Once the Perth discovery is complete and its data is being managed by the **Phase 2: Optimization** model, the learnings can be applied to the next target cluster (e.g., Adelaide).

### Proposed Data Model
To support the grouping logic, the database will be extended with two new models:
*   **`StoreGroup`:** Represents a group itself. It will have a name, a link to its parent company, and an `is_active` flag. It will also have two key relationships:
    *   `ambassador`: A nullable link (ForeignKey) to the single `Store` that is the current, official Ambassador for the group.
    *   `candidates`: A ManyToManyField linking to `Store`s that have been recently scraped and are awaiting a health check. This relationship is temporary and is cleared after each run.
*   **`StoreGroupMembership`:** A linking table that connects a `Store` to a `StoreGroup`. This will hold metadata about the membership, such as a `last_scraped_as_candidate` timestamp to help with rotating health checks. Outliers will be stores with no entry in this table.

### Dynamic Re-classification
The system must be dynamic. The "True Overlap" scores and group memberships must be re-calculated periodically to adapt to changes in store strategies. The comparison of direct vs. inferred data serves as the primary trigger for this re-evaluation.

### Scrape Scheduler Implementation
The core intelligence of the scraping process will be encapsulated in a dedicated `ScrapeScheduler` class. This class will be responsible for generating the queue of stores to be scraped in any given run. It will have two primary methods, corresponding to the two phases of the project:

*   **`get_discovery_mode_queue()`:** This method, which has already been designed, handles the initial discovery phase. It implements the 80/20 strategy by building a queue that is 80% focused on completing the largest unscraped geographic cluster and 20% focused on randomly exploring all other unscraped stores and outliers.

*   **`get_optimization_mode_queue()`:** This method will be developed for Phase 2. It will generate the daily scrape queue with a different set of priorities:
    1.  **Resolving Divergence:** First, it will queue any "tie-breaker" stores needed to resolve ambiguity in groups that have failed a health check.
    2.  **Maintaining Outliers:** Next, it will queue the outlier store that has the most outdated data.
    3.  **Routine Health Checks:** Finally, it will queue two rotating ambassadors from the healthy group that has gone the longest without a check.

## 5. Open Questions & Key Decisions

This strategy opens up several questions that will need to be answered during implementation:

1.  **Thresholds:** What is the initial "True Overlap" percentage required to form a group (95%, 98%)? What level of price deviation in a spot-check triggers a full re-scrape?
2.  **Re-analysis Frequency:** How often should the full, system-wide re-analysis of all groups be performed (e.g., weekly, monthly, quarterly)?
3.  **Ambassador Selection:** When rotating an ambassador, is the selection random, or round-robin?
6.  **Data Freshness:** What is the maximum acceptable age for data used in the "True Overlap" analysis? (e.g., must be from the same 24-hour period).
7.  **Future Performance Optimizations:** For the "re-homing" process, could we use more advanced techniques like data fingerprinting (e.g., MD5 hashing for perfect matches, or SimHash for near-matches) to accelerate finding candidate groups even faster than the two-stage check?