
# Scraping Process Refactor Plan

## 1. High-Level Goal

To refactor the web scraping process to be more robust, efficient, and atomic. This involves moving away from saving thousands of small JSON files for individual products and ensuring that a store's data is only saved if the entire scrape for that store is successful.

## 2. Problem Statement

The current scraping process has two main issues:

*   **Inefficiency:** Saving a separate JSON file for each product creates a significant I/O bottleneck. Processing these many small files is slow and inefficient.
*   **Lack of Atomicity:** The checkpoint system, while useful for interruptions, means that a store can be left in a partially scraped state. This can lead to incomplete or inconsistent data in the database. For most stores, it would be better to ensure that we only commit data from a complete and successful scrape. With the exception of coles. Coles needs checkpoints.

## 3. Proposed Solution

We will implement a new atomic scraping process that addresses these issues. The core idea is to build up a single data file for each store during the scrape and only move it to the final `product_inbox` upon successful completion.

### Key Changes:

*   ** File per Scrape:** For each store being scraped, a  JSON file will be created. All products found during the scrape will be appended to this file.
*   **Atomic Operation:** The scraping logic for each store will be wrapped in a `try...finally` block.
    *   **`try` block:** Contains the main scraping loop.
    *   **`finally` block:**
        *   If the scrape was successful, the  file is moved to the `product_inbox`.
        *   If the scrape was interrupted or failed, the  file is deleted.
*   **Coles Exception:** The existing checkpoint system will be maintained for Coles, as it is prone to interruptions where resuming is valuable. The Coles scraper will continue to write individual files to the inbox as it does now.
*   **Removal of Checkpoints:** The checkpointing logic (`read_checkpoint`, `update_page_progress`, etc.) will be removed from all other scrapers (Woolworths, Aldi, IGA).
*   **Data Format:** The  file will contain a list of JSON objects, where each object is a product. This will be a single large file that can be processed more efficiently by the `update_db` command.

## 4. Affected Files

The following files will need to be modified:

*   **`api/scrapers/scrape_and_save_woolworths.py`**: To remove checkpointing and implement the new  file logic.
*   **`api/scrapers/scrape_and_save_aldi.py`**: To remove checkpointing and implement the new  file logic.
*   **`api/scrapers/scrape_and_save_iga.py`**: To remove checkpointing and implement the new  file logic.
*   **`api/scrapers/scrape_and_save_spudshed.py`**: To remove checkpointing and implement the new  file logic.
*   **`api/scrapers/scrape_and_save_foodworks.py`**: To remove checkpointing and implement the new  file logic.
*   **`api/scrapers/scrape_and_save_friendly_grocer.py`**: To remove checkpointing and implement the new  file logic.
*   **`api/utils/scraper_utils/save_to_inbox.py`**: This will likely be replaced by a new utility function for writing to the  file. A new utility `finalize_scrape` will be needed to move the file.
*   **`api/utils/scraper_utils/checkpoint_utils/*`**: These files can be simplified or removed if they are no longer needed by any scraper other than Coles.
*   **`api/utils/database_updating_utils/consolidate_inbox_data.py`**: This will need to be updated to handle the new single-file-per-store format in the inbox. Instead of iterating through many small files, it will now process one larger file at a time.

## 5. Implementation Steps

1.  **Create New Utility Functions:**
    *   Create a function `append_to_temp_file(product_data, temp_file_path)` that appends a product's JSON data to the  file.
    *   Create a function `finalize_scrape(temp_file_path, inbox_path)` that moves the completed  file to the `product_inbox`.

2.  **Refactor Scrapers (Woolworths, Aldi, IGA, etc.):**
    *   For each scraper (except Coles), remove all calls to the checkpointing utilities.
    *   At the beginning of the main scraping function for a store, create a  file path.
    *   Wrap the main scraping loop in a `try...finally` block.
    *   Inside the loop, instead of calling `save_to_inbox`, call the new `append_to_temp_file` function.
    *   In the `finally` block, check a flag to see if the scrape was successful. If it was, call `finalize_scrape`. Otherwise, delete the  file.

3.  **Keep Coles Scraper As-Is:**
    *   Do not modify `scrape_and_save_coles.py` other than to ensure it continues to work with any changes to shared utilities.

4.  **Update `consolidate_inbox_data.py`:**
    *   Modify the logic to read the larger JSON files from the inbox. Instead of iterating over many small files, it will now open a single file and load the list of product objects from it.

5.  **Clean Up:**
    *   Remove the old `save_to_inbox.py` if it's no longer needed.
    *   Remove any unused checkpointing utility files.

    its very important that you do not change the scraping logic of the scrapers. They are very delicate. 
