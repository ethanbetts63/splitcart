00:01 foreign [Music]
00:16 so I work as a staff applied scientist at LD uh the topic for today's talk is
00:23 uh solving product substitutions using self-supervised machine learning
00:28 just the main agenda of the talk will we'll just touch on what's the
00:35 importance of substitutes and the grocery Commerce how low laws has collected human
00:42 annotated substitution data over time and how we are how we can use that data
00:47 to Benchmark the performance of different self-supervised models which we attempt
00:52 to learn in this experiments yeah so substitute products are Goods
00:59 which can be used interchangeably they have similar traits
01:05 for example a toothpaste can be by different brands having similar ingredients so that makes the product
01:12 substitute so in person when we are shopping it's
01:18 very we might not look into it but it's it's
01:23 very easy for us to make decisions on the fly if a product is not available we pick another thing but in grocery or in
01:31 e-commerce shopping in order to replicate similar experience we need to have a good substitutions
01:38 models underneath or even the ability to collect this
01:43 annotated data over time and we started with currently in lobla we
01:51 have a substitution model in which is running in all our products for especially for PC Express the topic for
01:57 today and uh and that's the process that the idea is to get
02:03 human labels over time using Active Learning so overall it it uh by solving
02:10 substitutions we improve the sales by minimizing the impact by out of stock orders improves user experience and
02:18 product discoverability where we can recommend products which are complement substitutes and
02:24 uh the reason this problem is a bit difficult is that we don't have any
02:30 ground Truth uh human annotated labels I would say we don't have human judgment
02:36 collected on these substitution products and the product catalogs can be large in
02:41 grocery it keeps on refreshing as well um
02:47 okay and bad recommendations for
02:52 substitutions they can erode Trust these yeah there are many tweets and if a
02:59 product substitution is not good it can leave a bad impact
03:05 So currently at lobla our substitution workflow is when when a user has
03:11 selected a card they have all the items in the cart there could be some items which are in low stock or out of stock
03:17 so when they are about to check out they're given an option if this product
03:23 is not available at certain pickup time or delivery time can we replace this and they can select
03:30 a product from those substitutions either fully bypass this and this is how we can also collect
03:36 human judgments so the very simple version of our first model which is running is it's using
03:43 lexical similarity from text which is tfid list similarity and pre-trained
03:51 image representation models and combining them with some business rules on top just so that we can get a
03:57 starting point to to serve recommendations and then over time collect data
04:03 so this model is running for some time and then in order to carry out more
04:08 experiments we need to have a benchmarking data set so that we can compare different models
04:14 performance against that so when someone places an order on the app for a pick
04:20 our colleagues they pick the order personally and you know make the card for someone who's gonna come
04:27 pick up and and in that process the many times when the user has not selected any
04:33 substitution they are the ones who make the choice of the substitution so that that has human judgment captured in it
04:40 and we have that data that when this product was not available What did the
04:46 personal shopper or the lobe law employee Shopper picked substitute for those products and using this we curated
04:53 a gold test set for product substitutions in grocery e-commerce it took last 18 months of data and we
04:59 prepared a data frame in this way that we have an
05:05 item code and again Stitch we have against it we have a list of items sorted in the order of their popularity
05:12 of substitution so we look at of all the substitutions what percentage of substitutions happen between the sphere
05:18 and then this where this way we have a salted
05:24 substitutions for a product based on their popularity some of the filters applied on the data especially just to
05:31 ensure that the gold test set is representative of good labels we're not
05:36 going to use this data for training we just the intention is to use them for benchmarking
05:42 uh yeah so once this data is curated for the benchmarking purposes we have 23 000
05:50 products different products in that data set as I explained like each row is a
05:55 product ID and against it we have the substitute and the popularity score
06:01 salted General products will have more substitutes and Niche products will have
06:06 less substitutes that's expected uh and this this distribution just shows
06:13 that of our actual catalog so these are different categories we
06:19 wanted to ensure that this goal set is capturing the same similar distribution of the actual catalog in terms of
06:27 percentage items and it it does to a good degree
06:32 this is some a visual example of how a goal set looks like once that's been prepared so the leftmost items are given
06:40 items are actual items and they are the substrates so the way it's looking now is like the the second
06:48 item is the most substituted item for the first item and then so on as I said
06:53 it's salted yeah just few examples this is on the test set against which
07:00 we'll measure our experiments so we did some qualitative evaluation to
07:05 just ensure it looks good and uh we can use it okay once we have this goal set prepared
07:13 the next uh we conducted some experiments self-supervised experiments
07:19 to to yeah to study can we capture substitution similar items this way
07:25 so in total in PC Express we haven't we have around 160k items not including the
07:32 marketplace so if you go on PC Express app other than Marketplace uh we curated all the items we have for
07:40 each for items we would see we have various Fields like ingredients text
07:46 brand name all those things and as well as images and then we carried out these three
07:53 experiments as a baseline model we used pre-trained bird generated bird
07:59 embeddings just as a baseline uh no pre no fine tuning done on it just that and
08:05 then we we did Mass language modeling uh by using Robert a and we trained our
08:12 own custom tokenizer as well and then for image representations we used
08:18 contrastive learning algorithm calls simclear uh it's a paper by Jeffrey
08:23 Hinton similar simple framework for learning contrastive representations
08:30 yeah okay so starting from Mass language modeling there are two ways to to do
08:37 Mass language modeling and Transformers uh we personally use Dragon Quest library but uh one way is you take a
08:45 pre-trained Model A language model and attach a masking LM head on top and continue
08:51 training it so that on the left side that's called domain adaptation I would
08:58 say and the the other side other way to train a language model from scratch is
09:04 that we train our own tokenizer first on our data which is fitted on our kind of
09:10 text not like all language all English language which really necessarily don't
09:16 need and then based on those steps we we create our own transformer
09:22 architecture in a way that we can modify the number of encoder layers embedding
09:27 Dimensions feed forward layer sizes and in the first approach when we just attach an LM head on top of Bird robot
09:33 or distal but the thing is like the tokenizer is still the same for bird and
09:38 that's been trained on English language so so even without text it's going to split them on
09:44 uh the training which happened based on but but uses word Feast tokenizer we
09:50 went with robotism that uses BP by byte Fair encoding and we'll get to it
09:58 so this is how for 160k items we prepared the text you would see some highlighted tokens
10:04 they they are special tokens which we will be using later in the masking we don't want these tokens to be masked
10:12 and in Mass language modeling a certain percentage of word tokens get masked and
10:17 the idea is we learned these associations between categories Brands how a name associated to a brand how a
10:23 category Associates to a brand and vice versa and then before uh
10:30 anything we the first step is to do some normalization on the text pre-tokenization and for this we just
10:37 did it lowercase yeah as you can see like all the decimals are split up digits stay
10:44 together but all the punctuations are isolated except for those special tokens so it's
10:50 important to ensure that like the special tokens they don't get separated
10:55 so that they get treated as one unit in the training so that's the first process we did these steps and we get text like
11:02 this then once we have this text we can train
11:08 okay I think it's we can train a tokenizer and it's a fitted tokenizer it
11:13 starts at the byte level and based on the frequency of our Corpus decides where to split words or not and and when
11:20 we have retail grocery it's very like niche in that area and uh we can utilize
11:27 it so we trained BP tokenizer using pre-processed text when we get this kind
11:33 of train tokenizer where the vocabulary size is 18 000 of the two even a word gets splits into smaller
11:39 words and that's the token vocabulary size special tokens the first four are internal tokens for robot a but we have
11:47 added some our own tokens uh will use in masking
11:52 and the maximum sequence length is 60 because we did an analysis and we we
11:57 tokenized all our data and saw different percentiles distribution and we realized
12:03 that 99 of percentile data is covered in 60 Max sequence lengths so we didn't
12:09 built has 5 112 Roberta has 5 112 so do we really need that one when we when
12:15 we're training from scratch in any way so these were some choices there and
12:20 just to have a comparison between Bird tokenized versus our custom tokenized tokenizer which we have trained you can
12:27 see this is the input that's Bert off the shelf and it's using
12:33 wordpiece tokenizer so it would split these words like aerosol whereas our
12:39 trained tokenizer doesn't split the word aerosol because it happens frequent enough in our retail domain and and it's
12:48 good for like we we don't want to split words where we don't need to and when the vocabulary size is limited to 18K
12:54 Roberta has 32k and 50K okay so then comes the point of uh
13:01 making the architecture for Mass language model Transformer so just a comparison between the the
13:08 default parameters or Roberta base so we are using 18K vocabulary our maximum
13:14 sequence length is 60 62. for that is 512. our output embedding dimension for
13:21 each learned token later on is 256 they have 768 so we'll that we started with a
13:29 smaller one and see like then expand rather than just take 12 layer encoder
13:34 and number of attention heads number of encoder layers these are like stacked
13:39 encoders we just used four other than 12 uh intermediate this is the feed forward
13:45 layer size within each encoder and then Roberta differs from bird and this way
13:51 that it does dynamic masking so we don't need to have like the data prepared beforehand rather when
13:57 we are when we have prepared a batch it dynamically masks random tokens with a
14:02 certain percentage and so with every Epoch different words would get masked for the same example
14:08 and we just pass in the corpus once this model is trained we can use
14:14 since we have trained this model for Mass language modeling so this is just an inference on
14:21 that exact task as you can see the the word after toys cat toys is masked and
14:28 the model has to predict and the first prediction is with which is right and so
14:33 this is just a qualitative evaluation of the mass language model trained on this
14:38 data okay so what now then once we have this Roberta model trained we haven't trained
14:47 it on any classification tasks towards categories but rather we have just trained it on mass language modeling
14:52 which has textual categorical information somehow and when we attach labels of different
14:59 categories which you internally have we see some clusters being formed which is a very good indication that our
15:05 embedding representations are you know they're they're far apart for dissimilar items and it's just a
15:14 qualitative way to evaluate the embeddings this is a 3D dimensional plot for and
15:21 you can see like clusters very well well tied clusters formed together
15:27 so it's like the text has a lot of information especially in grocery data Maybe not in fashion apparel but
15:33 especially here okay once this is trained we have these
15:39 plots and then yeah this is the trained Roberta only given there's left items
15:44 you get these recommendations or the similar items we are showing the images but underneath
15:50 it's using the text Data only just two have a quick evaluation
15:58 okay some more examples and we can see like some of the pictures
16:03 like the third example the second the the most similar item is visually very dissimilar but it's it must have been
16:11 textually similar okay so once we have this model which
16:16 has which using which we have learned the text representations the The Next
16:21 Step was we also have images and we have good quality of images in this way that
16:27 the background is always white and uh it's easy so we looked into
16:32 self-supervised algorithms for contrastive learning so contrastive learning is a is a self-supervised method where
16:39 we create augmentations for a given image contrasting images based on what we want to do and those pairs get
16:47 treated as positive labels in that batch and all the rest of the examples are treated as negative examples so we don't
16:53 need any label data uh and it's very similar to how you know puzzle can be solved as like matching
17:01 exact animal so for that we used sem clear uh it's uh
17:07 the full form is a simple framework for contrastable contrastive learning of visual representations it's a paper by
17:15 Jeffrey Hinton and uh it's using like there's a term metric learning contrastive learning being used so we
17:22 take any image encoder Inception V3 resnet resonate 15 and that's encoder
17:29 create two copies and the uh the the framework is that an original image
17:35 comes in we create augmentations which are positive Pairs and the rest of the batch becomes negative examples and then
17:41 the projection head which is the same Clear head at the end the loss function in there tries to maximize the
17:47 similarity uh for between the positive Pairs and dissimilarities so it's going to push apart
17:53 the the images which are not from the same class and bring the
17:58 yeah positive class labels closer together in the Learned space
18:04 and in this process like the the one thing which we decide or work on is
18:10 image augmentations what kind of augmentations we want to create and there are multiple ones where we can
18:16 do grayscale as well you would see we have we are not doing grayscale uh there
18:21 were some random flip horizontal flip but we are doing only these ones because
18:27 we also don't want to just totally lose color by doing grayscale uh
18:34 and when we're using Color Jitter as well it can totally change its colors so we we we have certain strengths using
18:42 which we can you know uh give a strength to the color jitters so these are some so when example comes
18:48 in these two would be positive Pairs and the rest of the batch becomes negative pairs in the training
18:54 yeah it's just uh resuming of training over all 500 epochs on our uh data and
19:01 we saw that after some time the Sinclair model is converged uh and the backbone model or the encoder
19:07 which you're using is resnet one six strained we just take that out and we can use it to generate embeddings
19:14 okay uh so so these are the embedding representations from simclear and as you
19:19 can see it's it's not very good at classifying different classes and
19:25 that could make sense because products could be visually very similar to each other and we'll see that whereas like
19:32 they're totally different uh in the textual modeling we are also capturing categories so that's better at
19:38 capturing those things okay now we have two models
19:44 so these are the neighbors generated by simclear only and and you'll notice that
19:49 visually they are similar but it could vary a lot from I don't
19:56 know what that yeah the third example after Cheerios you have salami uh
20:02 the second example you can see like visually the first two examples look same in the second example but they're very
20:09 different products so how about like we we have this uh
20:16 text based representations now and also image based representations the next
20:22 idea is to combine them uh together but uh yeah that's the next idea how can we
20:29 combine them now both are in different latent spaces one is 512 dimensional vectors one is 256
20:37 demand and concatenating it doesn't so we want to search the nearest neighbors
20:42 within the Learned space for each model and once we have learned the similarities then we weigh the
20:47 similarities by a certain weightage 0.8 to Roberta 0.2 to Sinclair and and
20:54 and see how we perform on the gold test set so to do this
20:60 we we already established that uh text representations are better at capturing
21:06 actual gist of the product so we use Roberta to generate top 2000 candidates
21:12 for each item based on Roberta similarity now once we have these 2000 candidates we also get
21:19 resnet or Sim clear based similarities for those items and re-rank those
21:25 candidates which we have chosen from Roberta so the first 2000 candidates it is a
21:31 variable number it's just because of some latency and saving memory we don't
21:38 actually need to go all the 160k and generate so we generate 2000 candidates from Roberta re-rank them by Sinclair
21:45 based voltages the metrics which we are using here are two uh Precision at k
21:52 in this way that we have a goal set and in the gold set for each item the number of substitutes can vary for one item
21:59 there are two for other it's ten So It's tricky to use ndcg or any kind
22:05 of that metric there so we are using Precision at K which is looking at the top substitute for an item and we want
22:11 to see that in how many recommendations we are able to capture the top substitute from the goal set using these
22:17 approaches mean reciprocal rank is like one by the rank capped at a certain point which we can do so this slide
22:26 captures it all in in the results on the gold test set on the x-axis we have K
22:32 since we are we can generate 160 never 160k enabled so we need to have an upper
22:38 cap so we we start with when we are Computing Matrix at k equal
22:43 to 2 we only look at top two recommendations and see if if it is that if it contains the top substitute from
22:50 the goal set and it would result in TP or F true positives false positives
22:55 okay uh so but just with the text I understand like the text we are using is not very
23:02 coherent it's not like doesn't have the coherence as language does which word is trained on
23:09 so we expect it to it's a flat line at the bottom that's burnt and then sem
23:15 clear which is image representations is that green line uh still not great and then the yellow line
23:24 is the Mass language model Roberta at the end it's the second most second from the
23:29 talk just text and then this these numbers point one point two point three is is
23:36 the weightage assigned to visual representations or visual similarities so point one means Roberta has 0.9
23:43 weightage assigned to the similarities calculated there and point one only two
23:48 SIM clear and then point two so we notice that as we keep on increasing the weightage of
23:54 visual representations in this case the performance keeps declining and the best we see I mean at these data
24:01 points is that if we just response like just a touch of an image or just a small
24:07 weightage assigned to the image but major weightage to the text representations
24:13 and the we see like point one there and and like later we have plans we have
24:19 a good goal test set which has multiple items so we can we can even account for that in our calculations it's just that
24:26 we'll uh yeah these are the plots for m r but they'll
24:31 follow a similar Trend in terms of performances
24:39 yeah so some of the future work ideas which we discuss and are working will be
24:45 to to start looking into language and image pre-training of models where they are
24:52 where the representations of image and text description is learned in the same shared space
24:58 uh there are there is work on that there's an interesting paper clip
25:05 which explores and this is an idea for future exploration and also in all this talk we would have
25:12 noticed we are all we're only capturing item to item relationship like we're not taking into account any user signals uh
25:19 and that's the idea of experiments but we can incorporate you know user
25:25 preferences and we all as Kai had a talk in the morning uh so we have those user
25:32 representations as well which we can incorporate later on and by this process and having personal
25:38 shoppers in store keep collecting human verified data
25:43 uh yeah it's very valuable the the only reason we can carry out these experiments and compare them is this is
25:50 because of the actual human selected data which we had
25:57 yet some of the acknowledgments a colleague of mine Brandon he worked on creating the gold test set for this he
26:03 yeah he's not here today and some references
26:09 each one is uh yeah I worked with him on like tailoring the experiments
26:17 thank you
26:25 [Music] not sure if
26:33 yeah any questions
26:42 hello oh here we go hi uh thanks so much for your talk
26:47 um I'm coming at this from a little bit of a different place because I too work for lobloss but not for the
26:53 day um a question for you about uh this happens at Loblaws and a lot of other retailers we already have an organized
26:60 taxonomy of products available so you we're showing mchy
27:05 [Music] why um have you ever been interested to take a look at
27:12 whether that existing taxonomy is either a good way of verifying your outputs or could be used as an input into deciding
27:19 substitution so for example we have Frozen mangoes and fresh mangoes those may be similar in terms of
27:27 textual similarity but for example they would be perhaps not product substitutes that the
27:33 that the Shopper would be interested in getting and those two are quite separate
27:38 overall retail taxonomy of how the articles are organized
27:44 yeah okay if I understand it so yes we do have and and we would know we have
27:49 multiple taxonomials too so uh in this and and as as it's a taxonomy so it has
27:56 different hierarchy levels so my like idea is rather than we can
28:02 use any we can flatten the hierarchy use those points as well but we didn't do that because I feel like there's a
28:08 there's a relation in that taxonomy like okay Center of store then this like a
28:14 president has to happen for the I think in this uh to your point how can we just in can
28:22 we just feed categories into this I think we are in a way because we are
28:27 doing masking and it's uh so sometimes the the categories get masked certain
28:33 and based on the name the model has to predict the category or vice versa
28:40 does that answer
28:48 there's a hint
28:56 hey um thanks for the talk was great uh just a question about
29:02 the similarity of products so I think if I remember correctly you're using uh k n
29:09 to like find similar products in your 256 or 512 dimensional space
29:16 um how does that work like like what's the distance metric you're
29:22 using yeah just asking because yeah we deal with similar types of data
29:28 so we're using K nearest neighbors since these are dense representations we we're using cosine similarity
29:34 uh for both
29:46 generation I just had a question about how you obeying the rankings that you got from the vision model and the
29:52 language model so he didn't get the first part so
29:60 I was like you have the language model which was ranking the similar some
30:05 of the vision model as well like ranking the similar products like how were you merging them together oh I see yeah I
30:11 think it was uh so so you know if we have an item to item
30:19 Matrix we can create a half triangular Matrix where given a pair of products
30:24 what's the similarity and that entire uh similarity metrics can be captured as
30:31 item by item Matrix we store that we do the same for Sinclair and when a query
30:37 comes in we get the similarities from both weigh them and generate the final similarity but in case of generating
30:43 recommendations we need to have like n as like 100 recommendations 10. so in
30:49 that case you know we generate from we get the top similar items from
30:54 Roberta once we have that we get the similarities for those from simclear and
30:60 then weigh those based on those weights which I show like if Roberta similarity is one after weight it becomes 0.9 and
31:09 same clearas would be weighted based on their weight and then the final results
31:14 get re-ranked again does that make sense
31:23 oh since we have two models so it's like you know a one or the other so we we did
31:28 a search on point one to point nine
32:02 no that's a great point and this is the idea of like before getting these results we would not know as well so so
32:09 now we know and like our decisions would be based on these things
32:15 it's not these are experiments uh all of these these are not models in production
32:26 okay thank you thanks very much