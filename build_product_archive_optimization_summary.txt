# Optimization Summary for build_product_archive.py

This document summarizes the two main phases of optimization applied to the `build_product_archive.py` utility, which is responsible for generating store-specific JSON files containing product and price data.

## Phase 1: Eliminating N+1 Queries for Category Paths

**Original Problem:**

*   The `build_product_list` function, when processing products for a store, was inefficiently generating category paths.
*   For each product, and for each of its categories, it would trigger new database queries to traverse the category hierarchy (`product.category.all()` and subsequent calls within `get_category_path`). This led to a classic N+1 query problem, causing thousands of small, slow database hits.

**Solution Implemented:**

1.  **New Utility Function:** Created `api/utils/archiving_utils/build_category_path_map.py`.
    *   This function is designed to efficiently build a map of category IDs to their full hierarchical paths by querying the database for all relevant categories and their parents in a single, optimized batch.
2.  **Refactored `build_product_list.py`:**
    *   Modified `build_product_list` to use this new utility.
    *   Instead of individual queries, it now gathers all unique category IDs for a chunk of products.
    *   It then calls `build_category_path_map` once per chunk to get all category paths in an efficient manner.
    *   Category path lookups for individual products became fast in-memory operations.

**Impact:** Significantly reduced the number of database queries, leading to a substantial speedup in processing each chunk of products.

## Phase 2: Resolving Memory Pressure and Performance Cliff

**Original Problem:**

*   After Phase 1, it was observed that the process was very fast for the first few thousand products but then slowed dramatically to a crawl.
*   This was diagnosed as a memory issue: the `all_parents_map` (the full category hierarchy) was being reloaded from the database into memory for *every single chunk* of products within a store.
*   Repeatedly loading this potentially large map caused memory pressure, leading the operating system to use swap space (hard drive as virtual memory), which is orders of magnitude slower than RAM.

**Solution Implemented:**

1.  **Refactored `build_category_path_map.py`:**
    *   Modified its signature to accept the `all_parents_map` as an argument, removing the logic that loaded it from within the function.
2.  **Refactored `build_product_list.py`:**
    *   Modified its signature to accept the `all_parents_map` and pass it to `build_category_path_map`.
3.  **Refactored `archive_single_store.py`:**
    *   This function (which processes a single store) was updated to load the `all_parents_map` from the database **only once** at the beginning of processing a store.
    *   This pre-loaded map is then passed down to `build_product_list` and `build_category_path_map` for all subsequent operations within that store.

**Impact:** Eliminated the repeated loading of the category hierarchy, resolving the memory pressure and the associated performance cliff. The process should now maintain its high speed consistently throughout the entire run for a store.

## Overall Impact

These optimizations collectively make the `archive --products` command (specifically the `build_product_archive` utility) much faster and more consistently performant, especially for large datasets with many products and categories.\



so this is an overview from the previous ai. Now we have tried a bunch of different methods. Some slightly faster some slightly slower. But they all get stuck in terms of speed around that 7500 mark. and alway at the first store it processes. So we need to figure out why. What is it about that data or the way we are calling it that is slowing down at that mark. it could be memory but i don't think so i think its something else. 
the store it gets stuck on is (2218/5683) Processing store: IGA Local Grocer Mt Gambier (Montebello) (14859)...

ive just looked and found that file is 7.5gb which it should not be all the other iga stores are something liek 50 mb at the most. So we need to figure out what is going on their too. 