Splitcart is a grocery price comparison application designed to help users find the best prices for their favorite products across different Australian grocery stores. To achieve this, it is crucial to accurately identify the same product even when it is listed with slight variations in different stores. This document outlines the new, streamlined data cleaning and de-duplication process, from raw data scraping to final database entry, reflecting the implemented changes.

**New Data Cleaning and De-duplication Pipeline**

**Part 1: Scraping and Initial Cleaning**

1.  **Scrape Raw Data:** The process begins by scraping the raw product data directly from the grocery store's API. This is initiated by a management command (e.g., `python manage.py scrape --woolworths`). The raw data for each page of products is returned as a JSON object.

2.  **Store-Specific Cleaning Function:** For each grocery store, a specialized cleaning function (e.g., `clean_raw_data_woolworths`) processes the raw JSON data. This function performs initial cleaning and normalization:
    *   **Data Extraction:** Extracts relevant values from the complex, nested JSON structure and maps them to standardized keys (e.g., `name`, `brand`, `price_current`).
    *   **URL Construction:** Builds a direct URL to the product page.
    *   **Tag Aggregation:** Combines various data points into a single list of tags.
    *   **Category Path Creation:** Constructs a hierarchical category path from available data.
    *   **Data Type Conversion:** Converts data to correct types (e.g., health star rating to float).

3.  **Generic Normalization (Unified and Enhanced):** After store-specific cleaning, the powerful `normalize_product_data` function (from `api/utils/normalization_utils`) is applied to each product. This function performs advanced normalization and generates the crucial de-duplication key:
    *   **Size Extraction and Standardization:** Extracts multiple size variations from product names and brands, standardizes units (e.g., '1 kg' becomes '1000g', '1 litre' becomes '1000ml'), and stores them in an `extracted_sizes` field. The original `package_size` is preserved.
    *   **Cleaned Name Generation:** Removes brand names and size information from the product name to create a `cleaned_name`.
    *   **De-duplication Key Generation:** Creates a `normalized_name_brand_size` key by combining the `cleaned_name`, `brand`, and `extracted_sizes` (all lowercased and stripped of non-alphanumeric characters). This key is now consistently used for de-duplication.

4.  **Save to Product Inbox:** Instead of saving page-level data, the scraper now saves a separate JSON file for **each individual product** into the `api/data/product_inbox/` directory. Each file contains the fully cleaned and normalized product data, including the `normalized_name_brand_size` key, along with relevant metadata.

**Part 2: Database Update and Integrated De-duplication**

5.  **Update Database (`update_db --products` command):** The `process_raw_data` command and the `processed_data` directory have been eliminated. The `update_db --products` command is now responsible for the entire product data processing and database update.

6.  **Read from Product Inbox:** The command reads all individual product JSON files directly from the `api/data/product_inbox/` directory.

7.  **In-Memory De-duplication:** All products from the inbox are loaded into memory and de-duplicated using their `normalized_name_brand_size` key. If multiple files represent the same product (based on this key), their data is consolidated (e.g., the most recent scrape's details are prioritized, and prices are accumulated).

8.  **Identify New vs. Existing Products:** The de-duplicated products are then matched against the existing products in the database using a tiered system:
    *   **Tier 1 (Barcode):** Highest priority. Checks for existing products by barcode.
    *   **Tier 2 (Store-Specific Product ID):** If no barcode match, checks for existing products by `store_product_id` for that specific store.
    *   **Tier 3 (Normalized Name-Brand-Size):** If no match from the above, it uses the pre-calculated, accurate `normalized_name_brand_size` key to find existing products. This is where the advanced normalization significantly improves cross-store de-duplication.

9.  **Batch Create New Products:** Products identified as new (no match in any tier) are collected and inserted into the `Product` model in a single, efficient batch operation.

10. **Create Prices:** New `Price` records are created for all products, linking them to the correct `Product` and `Store`.

11. **Create Category Relationships:** The script links products to their corresponding categories in the database.

12. **Clean Up Product Inbox:** Once all products from the inbox have been successfully processed and committed to the database, their corresponding individual JSON files are deleted from the `product_inbox` directory.

**Benefits of the New Hybrid Approach:**

*   **Simplified Pipeline:** The data flow is much more direct and efficient, eliminating intermediary steps and directories.
*   **Highly Accurate De-duplication:** By consistently using the advanced `normalized_name_brand_size` key throughout the pipeline, the system can accurately identify and consolidate products across different stores, even with variations in their raw data.
*   **Increased Resilience:** Processing individual product files means that an error with one product does not halt the processing of others.
*   **Preservation of Raw Data:** The original `package_size` is preserved, and normalized data is stored in a separate field, allowing for better auditing and future improvements.
*   **Unified Normalization Logic:** All normalization is handled by a single, powerful module, ensuring consistency and easier maintenance.
