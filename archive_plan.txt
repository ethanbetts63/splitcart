# Data Processing and Archiving Pipeline

This document outlines the current data processing pipeline, from raw data collection to the generation of final, sellable data archives.

## Implemented Stages

The pipeline consists of several automated stages that process raw data and populate the central database, followed by a process to generate company-level archives.

### Stage 1: Store Database Population
*   **Command:** `python manage.py update_store_database`
*   **Purpose:** This command processes raw JSON files containing store information from the `api/data/discovered_stores/` directory.
*   **Process:** It reads each file, creates or updates the corresponding `Company`, `Division`, and `Store` records in the database, and then deletes the source file. This ensures the central database always has the most up-to-date store information.

### Stage 2: Product Database Population
*   **Command:** `python manage.py update_database`
*   **Purpose:** This command populates the database with product and price information from processed data files located in `api/data/processed_data/`.
*   **Process:** For each file, it deactivates old prices for the relevant store, then creates or updates `Product` records and adds the new `Price` information. This command maintains a live, relational database of all products and their current prices.

### Stage 3: Company Archive Generation
*   **Command:** `python manage.py build_company_jsons`
*   **Purpose:** This command generates the first set of sellable data archives.
*   **Process:** It queries the database and creates aggregated JSON files for each company (e.g., `coles.json`, `woolworths.json`) in the `api/data/archive/company_data/` directory. These files contain detailed lists of all stores belonging to each company, grouped by division.

## Future Work: Product Archives

The final piece of the archiving system, the generation of per-store **product archives**, has not yet been built.

*   **Concept:** These archives will be individual JSON files for each store (e.g., `archive/store_data/coles/{store-slug}.json`).
*   **Contents:** Each file will contain:
    *   Metadata about the specific store.
    *   A complete list of all products available at that store.
    *   A full price history for each product at that store.
    *   A hierarchical representation of the store's specific product categories.

This represents the remaining work required to complete the full suite of planned sellable data products.

---

## Implementation Plan: `build_store_jsons` Command

This section outlines the plan for the `build_store_jsons` command, which is responsible for generating the per-store product archives.

### 1. Command Definition and Arguments

The command will be defined in `api/management/commands/build_store_jsons.py` and will support the following arguments:

*   **`--company <company_slug>`**: (Optional) Restricts processing to a single company.
*   **`--store <store_id>`**: (Optional) Restricts processing to a single store, identified by its unique `store_id`.
*   **Default Behavior**: When run without arguments, the command will process all active stores for all companies.

### 2. Core Logic and Process

The command will follow a four-step process:

**Step 1: Select Target Stores**
Based on the provided arguments, the command will build a queryset of stores. To ensure efficiency and minimize database calls, this query will use `prefetch_related` to fetch all associated product and price data upfront.

**Step 2: Iterate Through Stores**
The command will loop through each store in the selected queryset. For each store, it will generate a single JSON file.

**Step 3: Gather and Structure Data**
For each store, a Python dictionary will be constructed with the following structure:
*   **`metadata`**: Will contain store details (name, address, ID), a `data_generation_date`, and a reconstructed `category_hierarchy` specific to that store's product set.
*   **`products`**: A list where each item represents a product. Each product will include its details (name, description, etc.) and a `price_history` list containing all of its price records at that store, sorted by date.

**Step 4: Generate and Save JSON File**
The final dictionary will be saved as a JSON file to `api/data/archive/store_data/{company_slug}/{store_id}.json`. The command will be responsible for creating the necessary directories.
